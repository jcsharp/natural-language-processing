{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/train.tsv is already downloaded.\n",
      "File data/validation.tsv is already downloaded.\n",
      "File data/test.tsv is already downloaded.\n",
      "File data/test_embeddings.tsv is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. You need to download it by following this [link](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin\", binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    vec = np.zeros(dim)\n",
    "    n = 0\n",
    "    for word in question.split():\n",
    "        if word in embeddings:\n",
    "            vec += embeddings[word]\n",
    "            n += 1\n",
    "    if n > 1:\n",
    "        vec /= n\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.019293891059\n",
      "-0.0287272135417\n",
      "0.0460561116536\n",
      "0.0852593315972\n",
      "0.0243055555556\n",
      "-0.0729031032986\n",
      "0.0...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list with ranks for each duplicate (the best rank is 1, the worst is len(dup_ranks))\n",
    "        k: number of top-ranked elements\n",
    "\n",
    "        result: float number\n",
    "    \"\"\"\n",
    "    hc = 0\n",
    "    for r in dup_ranks:\n",
    "        if r <= k: hc += 1\n",
    "    return hc / len(dup_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list with ranks for each duplicate (the best rank is 1, the worst is len(dup_ranks))\n",
    "        k: number of top-ranked elements\n",
    "\n",
    "        result: float number\n",
    "    \"\"\"\n",
    "    hc = 0.0\n",
    "    for r in dup_ranks:\n",
    "        if r <= k: hc += 1.0 / np.log2(1 + r)\n",
    "    return hc / len(dup_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.815464876786\n",
      "0.5\n",
      "0.815464876786\n",
      "0.333333333333\n",
      "0.54364325119\n",
      "0.710309917857\n",
      "0.1\n",
      "0.16309297...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should upload *validation* corpus to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = [question_to_vec(question, embeddings, dim)]\n",
    "    y = [question_to_vec(q, embeddings, dim) for q in candidates]\n",
    "    sims = cosine_similarity(x, y)\n",
    "    result = [(sims[0][i], (i, candidates[i])) for i in range(len(y))]\n",
    "    result = sorted(result, key=lambda r: r[0], reverse=True)\n",
    "    pairs = [r[1] for r in result]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.212 | Hits@   1: 0.212\n",
      "DCG@   5: 0.267 | Hits@   5: 0.315\n",
      "DCG@  10: 0.282 | Hits@  10: 0.363\n",
      "DCG@ 100: 0.320 | Hits@ 100: 0.552\n",
      "DCG@ 500: 0.353 | Hits@ 500: 0.811\n",
      "DCG@1000: 0.373 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    prepared_validation.append([text_prepare(q) for q in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.310 | Hits@   1: 0.310\n",
      "DCG@   5: 0.380 | Hits@   5: 0.443\n",
      "DCG@  10: 0.397 | Hits@  10: 0.494\n",
      "DCG@ 100: 0.430 | Hits@ 100: 0.661\n",
      "DCG@ 500: 0.453 | Hits@ 500: 0.835\n",
      "DCG@1000: 0.470 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_file('data/train.tsv', 'data/train_prep.tsv')\n",
    "prepare_file('data/test.tsv', 'data/test_prep.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task W2VTokenizedRanks is: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n"
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = 'data/test_prep.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/train_prep.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/train_prep.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 64.5%  lr: 0.043814  loss: 0.010661  eta: 0h19m  tot: 0h2m50s  (12.9%) (0.1%)0.7%  lr: 0.049960  loss: 0.058766  eta: 0h23m  tot: 0h0m2s  (0.1%)0.9%  lr: 0.049950  loss: 0.054010  eta: 0h25m  tot: 0h0m2s  (0.2%)1.0%  lr: 0.049940  loss: 0.052031  eta: 0h24m  tot: 0h0m2s  (0.2%)1.1%  lr: 0.049940  loss: 0.050725  eta: 0h24m  tot: 0h0m3s  (0.2%)1.7%  lr: 0.049920  loss: 0.044856  eta: 0h23m  tot: 0h0m4s  (0.3%)m7s  (0.5%)3.0%  lr: 0.049780  loss: 0.035830  eta: 0h21m  tot: 0h0m7s  (0.6%)3.1%  lr: 0.049770  loss: 0.035458  eta: 0h21m  tot: 0h0m7s  (0.6%)4.8%  lr: 0.049560  loss: 0.029533  eta: 0h20m  tot: 0h0m11s  (1.0%)4.9%  lr: 0.049540  loss: 0.029302  eta: 0h20m  tot: 0h0m12s  (1.0%)%  lr: 0.049489  loss: 0.028056  eta: 0h20m  tot: 0h0m13s  (1.1%)%  lr: 0.049449  loss: 0.027351  eta: 0h20m  tot: 0h0m14s  (1.2%)6.1%  lr: 0.049429  loss: 0.026872  eta: 0h20m  tot: 0h0m15s  (1.2%)%  lr: 0.049429  loss: 0.026692  eta: 0h20m  tot: 0h0m15s  (1.2%)6.7%  lr: 0.049379  loss: 0.025835  eta: 0h20m  tot: 0h0m16s  (1.3%)0.025491  eta: 0h20m  tot: 0h0m17s  (1.4%)8.3%  lr: 0.049219  loss: 0.023541  eta: 0h20m  tot: 0h0m20s  (1.7%)8.4%  lr: 0.049219  loss: 0.023443  eta: 0h20m  tot: 0h0m20s  (1.7%)8.8%  lr: 0.049179  loss: 0.023002  eta: 0h20m  tot: 0h0m21s  (1.8%)9.3%  lr: 0.049089  loss: 0.022494  eta: 0h20m  tot: 0h0m23s  (1.9%)9.5%  lr: 0.049059  loss: 0.022327  eta: 0h20m  tot: 0h0m23s  (1.9%)10.6%  lr: 0.048929  loss: 0.021224  eta: 0h20m  tot: 0h0m26s  (2.1%)11.0%  lr: 0.048899  loss: 0.021003  eta: 0h20m  tot: 0h0m27s  (2.2%)11.1%  lr: 0.048899  loss: 0.020953  eta: 0h20m  tot: 0h0m27s  (2.2%)12.0%  lr: 0.048829  loss: 0.020374  eta: 0h19m  tot: 0h0m29s  (2.4%)12.2%  lr: 0.048819  loss: 0.020199  eta: 0h19m  tot: 0h0m30s  (2.4%)12.3%  lr: 0.048819  loss: 0.020179  eta: 0h19m  tot: 0h0m30s  (2.5%)12.4%  lr: 0.048809  loss: 0.020089  eta: 0h19m  tot: 0h0m30s  (2.5%)13.1%  lr: 0.048749  loss: 0.019739  eta: 0h19m  tot: 0h0m32s  (2.6%)%  lr: 0.048719  loss: 0.019606  eta: 0h20m  tot: 0h0m32s  (2.7%)13.5%  lr: 0.048689  loss: 0.019467  eta: 0h20m  tot: 0h0m33s  (2.7%)15.3%  lr: 0.048539  loss: 0.018430  eta: 0h20m  tot: 0h0m39s  (3.1%)15.7%  lr: 0.048489  loss: 0.018259  eta: 0h20m  tot: 0h0m39s  (3.1%)16.0%  lr: 0.048448  loss: 0.018048  eta: 0h20m  tot: 0h0m41s  (3.2%)17.6%  lr: 0.048238  loss: 0.017503  eta: 0h20m  tot: 0h0m46s  (3.5%)%  lr: 0.048238  loss: 0.017380  eta: 0h21m  tot: 0h0m46s  (3.6%)18.5%  lr: 0.048158  loss: 0.017144  eta: 0h21m  tot: 0h0m48s  (3.7%)18.5%  lr: 0.048138  loss: 0.017129  eta: 0h21m  tot: 0h0m49s  (3.7%)18.6%  lr: 0.048128  loss: 0.017106  eta: 0h21m  tot: 0h0m49s  (3.7%)19.4%  lr: 0.048068  loss: 0.016811  eta: 0h21m  tot: 0h0m52s  (3.9%)19.7%  lr: 0.048048  loss: 0.016725  eta: 0h21m  tot: 0h0m53s  (3.9%)20.3%  lr: 0.048018  loss: 0.016568  eta: 0h21m  tot: 0h0m54s  (4.1%)21.4%  lr: 0.047928  loss: 0.016192  eta: 0h21m  tot: 0h0m57s  (4.3%)%  lr: 0.047818  loss: 0.015979  eta: 0h21m  tot: 0h1m0s  (4.4%)22.8%  lr: 0.047778  loss: 0.015886  eta: 0h21m  tot: 0h1m1s  (4.6%)23.9%  lr: 0.047668  loss: 0.015484  eta: 0h21m  tot: 0h1m4s  (4.8%)24.4%  lr: 0.047618  loss: 0.015390  eta: 0h21m  tot: 0h1m5s  (4.9%)24.5%  lr: 0.047608  loss: 0.015372  eta: 0h21m  tot: 0h1m6s  (4.9%)24.7%  lr: 0.047598  loss: 0.015362  eta: 0h21m  tot: 0h1m6s  (4.9%)0.015347  eta: 0h21m  tot: 0h1m6s  (5.0%)25.0%  lr: 0.047538  loss: 0.015291  eta: 0h21m  tot: 0h1m7s  (5.0%)25.3%  lr: 0.047538  loss: 0.015214  eta: 0h21m  tot: 0h1m8s  (5.1%)26.2%  lr: 0.047457  loss: 0.014983  eta: 0h21m  tot: 0h1m10s  (5.2%)26.5%  lr: 0.047447  loss: 0.014923  eta: 0h21m  tot: 0h1m11s  (5.3%)27.2%  lr: 0.047357  loss: 0.014719  eta: 0h21m  tot: 0h1m13s  (5.4%)27.5%  lr: 0.047337  loss: 0.014665  eta: 0h21m  tot: 0h1m13s  (5.5%)28.4%  lr: 0.047257  loss: 0.014449  eta: 0h21m  tot: 0h1m16s  (5.7%)0.014351  eta: 0h21m  tot: 0h1m18s  (5.8%)%  lr: 0.047167  loss: 0.014309  eta: 0h21m  tot: 0h1m19s  (5.9%)30.4%  lr: 0.047117  loss: 0.014203  eta: 0h20m  tot: 0h1m21s  (6.1%)30.6%  lr: 0.047117  loss: 0.014175  eta: 0h20m  tot: 0h1m22s  (6.1%)31.0%  lr: 0.047057  loss: 0.014102  eta: 0h20m  tot: 0h1m23s  (6.2%)31.2%  lr: 0.047037  loss: 0.014032  eta: 0h20m  tot: 0h1m23s  (6.2%)6.4%)32.5%  lr: 0.046977  loss: 0.013808  eta: 0h20m  tot: 0h1m26s  (6.5%)33.2%  lr: 0.046937  loss: 0.013702  eta: 0h20m  tot: 0h1m28s  (6.6%)34.7%  lr: 0.046837  loss: 0.013444  eta: 0h20m  tot: 0h1m32s  (6.9%)35.1%  lr: 0.046757  loss: 0.013374  eta: 0h20m  tot: 0h1m34s  (7.0%)37.2%  lr: 0.046537  loss: 0.013087  eta: 0h20m  tot: 0h1m40s  (7.4%)37.4%  lr: 0.046517  loss: 0.013066  eta: 0h20m  tot: 0h1m41s  (7.5%)37.4%  lr: 0.046507  loss: 0.013047  eta: 0h20m  tot: 0h1m41s  (7.5%)37.5%  lr: 0.046497  loss: 0.013035  eta: 0h20m  tot: 0h1m41s  (7.5%)38.2%  lr: 0.046447  loss: 0.012951  eta: 0h20m  tot: 0h1m43s  (7.6%)38.3%  lr: 0.046396  loss: 0.012945  eta: 0h20m  tot: 0h1m43s  (7.7%)39.3%  lr: 0.046286  loss: 0.012824  eta: 0h20m  tot: 0h1m46s  (7.9%)39.5%  lr: 0.046286  loss: 0.012806  eta: 0h20m  tot: 0h1m46s  (7.9%)40.0%  lr: 0.046256  loss: 0.012748  eta: 0h20m  tot: 0h1m48s  (8.0%)40.3%  lr: 0.046216  loss: 0.012704  eta: 0h20m  tot: 0h1m48s  (8.1%)40.8%  lr: 0.046186  loss: 0.012649  eta: 0h20m  tot: 0h1m50s  (8.2%)41.9%  lr: 0.046086  loss: 0.012498  eta: 0h20m  tot: 0h1m52s  (8.4%)43.6%  lr: 0.045936  loss: 0.012319  eta: 0h20m  tot: 0h1m57s  (8.7%)43.8%  lr: 0.045916  loss: 0.012300  eta: 0h20m  tot: 0h1m57s  (8.8%)44.0%  lr: 0.045866  loss: 0.012282  eta: 0h20m  tot: 0h1m58s  (8.8%)44.6%  lr: 0.045846  loss: 0.012220  eta: 0h20m  tot: 0h1m59s  (8.9%)45.4%  lr: 0.045746  loss: 0.012127  eta: 0h20m  tot: 0h2m1s  (9.1%)45.5%  lr: 0.045746  loss: 0.012116  eta: 0h20m  tot: 0h2m1s  (9.1%)45.6%  lr: 0.045746  loss: 0.012107  eta: 0h20m  tot: 0h2m2s  (9.1%)45.6%  lr: 0.045746  loss: 0.012097  eta: 0h20m  tot: 0h2m2s  (9.1%)45.8%  lr: 0.045716  loss: 0.012076  eta: 0h20m  tot: 0h2m2s  (9.2%)45.9%  lr: 0.045676  loss: 0.012068  eta: 0h20m  tot: 0h2m3s  (9.2%)46.3%  lr: 0.045626  loss: 0.012027  eta: 0h20m  tot: 0h2m4s  (9.3%)46.6%  lr: 0.045596  loss: 0.012012  eta: 0h20m  tot: 0h2m5s  (9.3%)47.3%  lr: 0.045556  loss: 0.011965  eta: 0h20m  tot: 0h2m7s  (9.5%)48.0%  lr: 0.045476  loss: 0.011886  eta: 0h20m  tot: 0h2m9s  (9.6%)48.1%  lr: 0.045466  loss: 0.011877  eta: 0h20m  tot: 0h2m9s  (9.6%)48.9%  lr: 0.045395  loss: 0.011814  eta: 0h20m  tot: 0h2m11s  (9.8%)49.8%  lr: 0.045245  loss: 0.011745  eta: 0h20m  tot: 0h2m13s  (10.0%)49.9%  lr: 0.045215  loss: 0.011739  eta: 0h20m  tot: 0h2m13s  (10.0%)50.9%  lr: 0.045155  loss: 0.011661  eta: 0h20m  tot: 0h2m16s  (10.2%)51.0%  lr: 0.045145  loss: 0.011653  eta: 0h20m  tot: 0h2m16s  (10.2%)51.8%  lr: 0.045065  loss: 0.011590  eta: 0h19m  tot: 0h2m18s  (10.4%)52.4%  lr: 0.045005  loss: 0.011532  eta: 0h19m  tot: 0h2m20s  (10.5%)53.2%  lr: 0.044945  loss: 0.011498  eta: 0h19m  tot: 0h2m22s  (10.6%)%  lr: 0.044725  loss: 0.011293  eta: 0h19m  tot: 0h2m27s  (11.1%)55.9%  lr: 0.044715  loss: 0.011268  eta: 0h19m  tot: 0h2m28s  (11.2%)57.3%  lr: 0.044585  loss: 0.011192  eta: 0h19m  tot: 0h2m31s  (11.5%)57.7%  lr: 0.044565  loss: 0.011150  eta: 0h19m  tot: 0h2m32s  (11.5%)58.2%  lr: 0.044515  loss: 0.011111  eta: 0h19m  tot: 0h2m33s  (11.6%)58.3%  lr: 0.044505  loss: 0.011113  eta: 0h19m  tot: 0h2m34s  (11.7%)58.4%  lr: 0.044505  loss: 0.011112  eta: 0h19m  tot: 0h2m34s  (11.7%)58.9%  lr: 0.044475  loss: 0.011068  eta: 0h19m  tot: 0h2m35s  (11.8%)59.0%  lr: 0.044475  loss: 0.011059  eta: 0h19m  tot: 0h2m35s  (11.8%)59.3%  lr: 0.044435  loss: 0.011031  eta: 0h19m  tot: 0h2m36s  (11.9%)60.0%  lr: 0.044374  loss: 0.010978  eta: 0h19m  tot: 0h2m38s  (12.0%)60.1%  lr: 0.044364  loss: 0.010966  eta: 0h19m  tot: 0h2m38s  (12.0%)%  lr: 0.044294  loss: 0.010950  eta: 0h19m  tot: 0h2m39s  (12.1%)60.8%  lr: 0.044204  loss: 0.010912  eta: 0h19m  tot: 0h2m40s  (12.2%)60.9%  lr: 0.044204  loss: 0.010895  eta: 0h19m  tot: 0h2m41s  (12.2%)61.4%  lr: 0.044154  loss: 0.010856  eta: 0h19m  tot: 0h2m42s  (12.3%)62.1%  lr: 0.044074  loss: 0.010819  eta: 0h19m  tot: 0h2m44s  (12.4%)62.9%  lr: 0.044004  loss: 0.010764  eta: 0h19m  tot: 0h2m46s  (12.6%)64.6%  lr: 0.043814  loss: 0.010658  eta: 0h19m  tot: 0h2m51s  (12.9%)Epoch: 100.0%  lr: 0.040200  loss: 0.009006  eta: 0h17m  tot: 0h4m23s  (20.0%)5.4%  lr: 0.043704  loss: 0.010603  eta: 0h19m  tot: 0h2m52s  (13.1%)66.6%  lr: 0.043564  loss: 0.010523  eta: 0h19m  tot: 0h2m56s  (13.3%)67.0%  lr: 0.043544  loss: 0.010493  eta: 0h19m  tot: 0h2m56s  (13.4%)68.0%  lr: 0.043474  loss: 0.010435  eta: 0h18m  tot: 0h2m59s  (13.6%)69.1%  lr: 0.043313  loss: 0.010351  eta: 0h18m  tot: 0h3m1s  (13.8%)69.9%  lr: 0.043243  loss: 0.010302  eta: 0h18m  tot: 0h3m3s  (14.0%)72.1%  lr: 0.043023  loss: 0.010189  eta: 0h18m  tot: 0h3m9s  (14.4%)73.6%  lr: 0.042893  loss: 0.010111  eta: 0h18m  tot: 0h3m13s  (14.7%)74.7%  lr: 0.042783  loss: 0.010053  eta: 0h18m  tot: 0h3m16s  (14.9%)75.2%  lr: 0.042753  loss: 0.010041  eta: 0h18m  tot: 0h3m17s  (15.0%)76.4%  lr: 0.042633  loss: 0.009966  eta: 0h18m  tot: 0h3m20s  (15.3%)77.0%  lr: 0.042613  loss: 0.009936  eta: 0h18m  tot: 0h3m22s  (15.4%)77.3%  lr: 0.042573  loss: 0.009927  eta: 0h18m  tot: 0h3m23s  (15.5%)77.7%  lr: 0.042543  loss: 0.009912  eta: 0h18m  tot: 0h3m24s  (15.5%)77.8%  lr: 0.042543  loss: 0.009908  eta: 0h18m  tot: 0h3m24s  (15.6%)78.7%  lr: 0.042473  loss: 0.009859  eta: 0h18m  tot: 0h3m26s  (15.7%)79.0%  lr: 0.042473  loss: 0.009840  eta: 0h18m  tot: 0h3m27s  (15.8%)79.1%  lr: 0.042463  loss: 0.009834  eta: 0h18m  tot: 0h3m27s  (15.8%)79.2%  lr: 0.042453  loss: 0.009828  eta: 0h18m  tot: 0h3m28s  (15.8%)79.3%  lr: 0.042433  loss: 0.009826  eta: 0h18m  tot: 0h3m28s  (15.9%)79.4%  lr: 0.042413  loss: 0.009823  eta: 0h18m  tot: 0h3m28s  (15.9%)79.9%  lr: 0.042393  loss: 0.009812  eta: 0h18m  tot: 0h3m29s  (16.0%)80.1%  lr: 0.042362  loss: 0.009808  eta: 0h18m  tot: 0h3m30s  (16.0%)81.0%  lr: 0.042272  loss: 0.009768  eta: 0h18m  tot: 0h3m32s  (16.2%)81.1%  lr: 0.042272  loss: 0.009761  eta: 0h18m  tot: 0h3m33s  (16.2%)81.3%  lr: 0.042242  loss: 0.009751  eta: 0h18m  tot: 0h3m33s  (16.3%)81.4%  lr: 0.042222  loss: 0.009745  eta: 0h18m  tot: 0h3m33s  (16.3%)%  lr: 0.042212  loss: 0.009738  eta: 0h18m  tot: 0h3m34s  (16.3%)81.6%  lr: 0.042212  loss: 0.009735  eta: 0h18m  tot: 0h3m34s  (16.3%)82.3%  lr: 0.042122  loss: 0.009694  eta: 0h18m  tot: 0h3m36s  (16.5%)84.1%  lr: 0.041972  loss: 0.009615  eta: 0h18m  tot: 0h3m40s  (16.8%)85.8%  lr: 0.041782  loss: 0.009549  eta: 0h18m  tot: 0h3m44s  (17.2%)85.9%  lr: 0.041782  loss: 0.009545  eta: 0h18m  tot: 0h3m45s  (17.2%)86.2%  lr: 0.041742  loss: 0.009536  eta: 0h18m  tot: 0h3m45s  (17.2%)86.3%  lr: 0.041712  loss: 0.009529  eta: 0h18m  tot: 0h3m46s  (17.3%)86.5%  lr: 0.041682  loss: 0.009516  eta: 0h18m  tot: 0h3m46s  (17.3%)87.4%  lr: 0.041562  loss: 0.009478  eta: 0h18m  tot: 0h3m49s  (17.5%)87.8%  lr: 0.041502  loss: 0.009460  eta: 0h18m  tot: 0h3m50s  (17.6%)88.6%  lr: 0.041432  loss: 0.009422  eta: 0h17m  tot: 0h3m51s  (17.7%)88.9%  lr: 0.041402  loss: 0.009407  eta: 0h17m  tot: 0h3m52s  (17.8%)89.1%  lr: 0.041382  loss: 0.009399  eta: 0h17m  tot: 0h3m53s  (17.8%)89.8%  lr: 0.041271  loss: 0.009369  eta: 0h17m  tot: 0h3m55s  (18.0%)89.9%  lr: 0.041261  loss: 0.009364  eta: 0h17m  tot: 0h3m55s  (18.0%)%  lr: 0.041211  loss: 0.009350  eta: 0h17m  tot: 0h3m56s  (18.1%)91.0%  lr: 0.041131  loss: 0.009326  eta: 0h17m  tot: 0h3m58s  (18.2%)91.6%  lr: 0.041101  loss: 0.009303  eta: 0h17m  tot: 0h3m59s  (18.3%)18.4%)%)94.2%  lr: 0.040741  loss: 0.009206  eta: 0h17m  tot: 0h4m8s  (18.8%)94.8%  lr: 0.040721  loss: 0.009179  eta: 0h17m  tot: 0h4m9s  (19.0%)97.4%  lr: 0.040421  loss: 0.009086  eta: 0h17m  tot: 0h4m16s  (19.5%)97.5%  lr: 0.040411  loss: 0.009081  eta: 0h17m  tot: 0h4m17s  (19.5%)99.7%  lr: 0.040230  loss: 0.009013  eta: 0h17m  tot: 0h4m23s  (19.9%)\n",
      " ---+++                Epoch    0 Train error : 0.00895619 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67.4%  lr: 0.033474  loss: 0.002742  eta: 0h16m  tot: 0h7m46s  (33.5%).3%  lr: 0.039930  loss: 0.002109  eta: 0h16m  tot: 0h4m34s  (20.3%)1.8%  lr: 0.039900  loss: 0.002197  eta: 0h17m  tot: 0h4m35s  (20.4%)2.8%  lr: 0.039850  loss: 0.002536  eta: 0h18m  tot: 0h4m38s  (20.6%)3.1%  lr: 0.039810  loss: 0.002465  eta: 0h17m  tot: 0h4m39s  (20.6%)%  lr: 0.039740  loss: 0.002672  eta: 0h18m  tot: 0h4m42s  (20.8%)4.2%  lr: 0.039730  loss: 0.002650  eta: 0h18m  tot: 0h4m42s  (20.8%)%  lr: 0.039730  loss: 0.002657  eta: 0h18m  tot: 0h4m43s  (20.9%)5.0%  lr: 0.039690  loss: 0.002648  eta: 0h18m  tot: 0h4m45s  (21.0%)5.2%  lr: 0.039620  loss: 0.002654  eta: 0h18m  tot: 0h4m45s  (21.0%)%  lr: 0.039510  loss: 0.002646  eta: 0h18m  tot: 0h4m48s  (21.2%)6.8%  lr: 0.039469  loss: 0.002624  eta: 0h18m  tot: 0h4m50s  (21.4%)7.1%  lr: 0.039459  loss: 0.002633  eta: 0h18m  tot: 0h4m51s  (21.4%)7.8%  lr: 0.039399  loss: 0.002574  eta: 0h18m  tot: 0h4m53s  (21.6%)7.9%  lr: 0.039389  loss: 0.002570  eta: 0h18m  tot: 0h4m53s  (21.6%)%  lr: 0.039289  loss: 0.002645  eta: 0h18m  tot: 0h4m56s  (21.8%)9.1%  lr: 0.039259  loss: 0.002657  eta: 0h18m  tot: 0h4m57s  (21.8%)9.2%  lr: 0.039239  loss: 0.002658  eta: 0h18m  tot: 0h4m57s  (21.8%)9.7%  lr: 0.039199  loss: 0.002714  eta: 0h18m  tot: 0h4m59s  (21.9%)10.4%  lr: 0.039109  loss: 0.002708  eta: 0h18m  tot: 0h5m1s  (22.1%)11.4%  lr: 0.038969  loss: 0.002770  eta: 0h18m  tot: 0h5m4s  (22.3%)12.3%  lr: 0.038839  loss: 0.002774  eta: 0h19m  tot: 0h5m7s  (22.5%)%  lr: 0.038749  loss: 0.002749  eta: 0h19m  tot: 0h5m10s  (22.7%)13.9%  lr: 0.038729  loss: 0.002760  eta: 0h19m  tot: 0h5m11s  (22.8%)14.0%  lr: 0.038709  loss: 0.002768  eta: 0h19m  tot: 0h5m12s  (22.8%)14.3%  lr: 0.038679  loss: 0.002744  eta: 0h18m  tot: 0h5m13s  (22.9%)14.8%  lr: 0.038589  loss: 0.002766  eta: 0h19m  tot: 0h5m14s  (23.0%)15.1%  lr: 0.038559  loss: 0.002786  eta: 0h19m  tot: 0h5m16s  (23.0%)16.1%  lr: 0.038499  loss: 0.002755  eta: 0h19m  tot: 0h5m19s  (23.2%)16.7%  lr: 0.038428  loss: 0.002760  eta: 0h19m  tot: 0h5m20s  (23.3%)23.5%)18.2%  lr: 0.038258  loss: 0.002736  eta: 0h18m  tot: 0h5m25s  (23.6%)18.3%  lr: 0.038248  loss: 0.002734  eta: 0h18m  tot: 0h5m25s  (23.7%)19.5%  lr: 0.038118  loss: 0.002725  eta: 0h18m  tot: 0h5m29s  (23.9%)21.2%  lr: 0.037948  loss: 0.002706  eta: 0h18m  tot: 0h5m34s  (24.2%)21.8%  lr: 0.037868  loss: 0.002685  eta: 0h18m  tot: 0h5m35s  (24.4%)%  lr: 0.037858  loss: 0.002685  eta: 0h18m  tot: 0h5m36s  (24.4%)22.6%  lr: 0.037808  loss: 0.002670  eta: 0h18m  tot: 0h5m38s  (24.5%)23.4%  lr: 0.037728  loss: 0.002672  eta: 0h18m  tot: 0h5m40s  (24.7%)24.0%  lr: 0.037658  loss: 0.002692  eta: 0h18m  tot: 0h5m42s  (24.8%)25.7%  lr: 0.037457  loss: 0.002727  eta: 0h18m  tot: 0h5m46s  (25.1%)26.3%  lr: 0.037427  loss: 0.002719  eta: 0h18m  tot: 0h5m48s  (25.3%)26.5%  lr: 0.037387  loss: 0.002731  eta: 0h18m  tot: 0h5m49s  (25.3%)27.2%  lr: 0.037347  loss: 0.002730  eta: 0h18m  tot: 0h5m51s  (25.4%)27.3%  lr: 0.037347  loss: 0.002727  eta: 0h18m  tot: 0h5m51s  (25.5%)27.8%  lr: 0.037267  loss: 0.002738  eta: 0h18m  tot: 0h5m53s  (25.6%)s  (25.6%)28.0%  lr: 0.037267  loss: 0.002740  eta: 0h18m  tot: 0h5m54s  (25.6%)%  lr: 0.037257  loss: 0.002748  eta: 0h18m  tot: 0h5m55s  (25.6%)28.4%  lr: 0.037227  loss: 0.002749  eta: 0h18m  tot: 0h5m55s  (25.7%) (25.9%)30.9%  lr: 0.036967  loss: 0.002727  eta: 0h18m  tot: 0h6m3s  (26.2%)31.1%  lr: 0.036947  loss: 0.002729  eta: 0h18m  tot: 0h6m4s  (26.2%)31.1%  lr: 0.036947  loss: 0.002727  eta: 0h18m  tot: 0h6m4s  (26.2%)31.2%  lr: 0.036947  loss: 0.002723  eta: 0h18m  tot: 0h6m5s  (26.2%)33.0%  lr: 0.036737  loss: 0.002730  eta: 0h18m  tot: 0h6m12s  (26.6%)h18m  tot: 0h6m14s  (26.8%)%  lr: 0.036637  loss: 0.002722  eta: 0h18m  tot: 0h6m15s  (26.8%)34.5%  lr: 0.036607  loss: 0.002727  eta: 0h18m  tot: 0h6m16s  (26.9%)35.2%  lr: 0.036547  loss: 0.002737  eta: 0h18m  tot: 0h6m18s  (27.0%)  lr: 0.036497  loss: 0.002751  eta: 0h18m  tot: 0h6m20s  (27.2%)36.3%  lr: 0.036477  loss: 0.002757  eta: 0h18m  tot: 0h6m22s  (27.3%)36.4%  lr: 0.036457  loss: 0.002752  eta: 0h18m  tot: 0h6m23s  (27.3%)0.002747  eta: 0h18m  tot: 0h6m23s  (27.3%)36.7%  lr: 0.036436  loss: 0.002739  eta: 0h18m  tot: 0h6m24s  (27.3%)m24s  (27.4%)39.1%  lr: 0.036206  loss: 0.002714  eta: 0h18m  tot: 0h6m31s  (27.8%)39.3%  lr: 0.036186  loss: 0.002725  eta: 0h18m  tot: 0h6m31s  (27.9%)39.8%  lr: 0.036166  loss: 0.002731  eta: 0h18m  tot: 0h6m33s  (28.0%)40.1%  lr: 0.036146  loss: 0.002727  eta: 0h18m  tot: 0h6m34s  (28.0%)40.5%  lr: 0.036116  loss: 0.002728  eta: 0h18m  tot: 0h6m35s  (28.1%)41.0%  lr: 0.036096  loss: 0.002718  eta: 0h18m  tot: 0h6m36s  (28.2%)41.2%  lr: 0.036066  loss: 0.002716  eta: 0h18m  tot: 0h6m37s  (28.2%)42.2%  lr: 0.035906  loss: 0.002708  eta: 0h18m  tot: 0h6m39s  (28.4%)42.6%  lr: 0.035866  loss: 0.002709  eta: 0h18m  tot: 0h6m40s  (28.5%)43.0%  lr: 0.035856  loss: 0.002716  eta: 0h18m  tot: 0h6m41s  (28.6%)43.7%  lr: 0.035846  loss: 0.002721  eta: 0h18m  tot: 0h6m44s  (28.7%)43.8%  lr: 0.035846  loss: 0.002719  eta: 0h18m  tot: 0h6m44s  (28.8%)44.0%  lr: 0.035826  loss: 0.002721  eta: 0h18m  tot: 0h6m45s  (28.8%)44.7%  lr: 0.035776  loss: 0.002715  eta: 0h18m  tot: 0h6m46s  (28.9%)45.0%  lr: 0.035736  loss: 0.002711  eta: 0h17m  tot: 0h6m47s  (29.0%)48.1%  lr: 0.035436  loss: 0.002713  eta: 0h17m  tot: 0h6m56s  (29.6%)48.2%  lr: 0.035436  loss: 0.002712  eta: 0h17m  tot: 0h6m56s  (29.6%)48.3%  lr: 0.035415  loss: 0.002714  eta: 0h17m  tot: 0h6m56s  (29.7%)48.9%  lr: 0.035355  loss: 0.002711  eta: 0h17m  tot: 0h6m57s  (29.8%)50.3%  lr: 0.035235  loss: 0.002720  eta: 0h17m  tot: 0h7m1s  (30.1%)51.1%  lr: 0.035125  loss: 0.002720  eta: 0h17m  tot: 0h7m3s  (30.2%)51.9%  lr: 0.035005  loss: 0.002727  eta: 0h17m  tot: 0h7m5s  (30.4%)51.9%  lr: 0.034995  loss: 0.002729  eta: 0h17m  tot: 0h7m6s  (30.4%)  lr: 0.034985  loss: 0.002729  eta: 0h17m  tot: 0h7m6s  (30.4%)52.2%  lr: 0.034975  loss: 0.002729  eta: 0h17m  tot: 0h7m6s  (30.4%)52.4%  lr: 0.034965  loss: 0.002734  eta: 0h17m  tot: 0h7m7s  (30.5%)53.5%  lr: 0.034855  loss: 0.002725  eta: 0h17m  tot: 0h7m10s  (30.7%)56.1%  lr: 0.034655  loss: 0.002719  eta: 0h16m  tot: 0h7m17s  (31.2%)56.2%  lr: 0.034645  loss: 0.002720  eta: 0h16m  tot: 0h7m17s  (31.2%)56.3%  lr: 0.034645  loss: 0.002719  eta: 0h16m  tot: 0h7m17s  (31.3%)57.2%  lr: 0.034635  loss: 0.002731  eta: 0h16m  tot: 0h7m19s  (31.4%)57.3%  lr: 0.034635  loss: 0.002733  eta: 0h16m  tot: 0h7m19s  (31.5%)57.6%  lr: 0.034585  loss: 0.002735  eta: 0h16m  tot: 0h7m20s  (31.5%)57.8%  lr: 0.034555  loss: 0.002732  eta: 0h16m  tot: 0h7m21s  (31.6%)58.2%  lr: 0.034505  loss: 0.002728  eta: 0h16m  tot: 0h7m22s  (31.6%)58.3%  lr: 0.034485  loss: 0.002726  eta: 0h16m  tot: 0h7m22s  (31.7%)59.4%  lr: 0.034435  loss: 0.002736  eta: 0h16m  tot: 0h7m25s  (31.9%)59.7%  lr: 0.034394  loss: 0.002745  eta: 0h16m  tot: 0h7m26s  (31.9%)60.3%  lr: 0.034284  loss: 0.002751  eta: 0h16m  tot: 0h7m27s  (32.1%)60.4%  lr: 0.034284  loss: 0.002752  eta: 0h16m  tot: 0h7m27s  (32.1%)60.5%  lr: 0.034274  loss: 0.002752  eta: 0h16m  tot: 0h7m28s  (32.1%)60.9%  lr: 0.034204  loss: 0.002754  eta: 0h16m  tot: 0h7m29s  (32.2%)%  lr: 0.034084  loss: 0.002747  eta: 0h16m  tot: 0h7m32s  (32.4%)62.1%  lr: 0.034074  loss: 0.002749  eta: 0h16m  tot: 0h7m32s  (32.4%)62.5%  lr: 0.034064  loss: 0.002750  eta: 0h16m  tot: 0h7m33s  (32.5%)62.8%  lr: 0.034054  loss: 0.002751  eta: 0h16m  tot: 0h7m34s  (32.6%)63.6%  lr: 0.033944  loss: 0.002752  eta: 0h16m  tot: 0h7m36s  (32.7%)63.6%  lr: 0.033934  loss: 0.002750  eta: 0h16m  tot: 0h7m36s  (32.7%)63.9%  lr: 0.033914  loss: 0.002749  eta: 0h16m  tot: 0h7m37s  (32.8%)64.1%  lr: 0.033884  loss: 0.002748  eta: 0h16m  tot: 0h7m37s  (32.8%)64.4%  lr: 0.033834  loss: 0.002744  eta: 0h16m  tot: 0h7m38s  (32.9%)%  lr: 0.033744  loss: 0.002740  eta: 0h16m  tot: 0h7m40s  (33.0%)65.3%  lr: 0.033714  loss: 0.002739  eta: 0h16m  tot: 0h7m40s  (33.1%)66.2%  lr: 0.033604  loss: 0.002738  eta: 0h16m  tot: 0h7m43s  (33.2%)67.0%  lr: 0.033534  loss: 0.002742  eta: 0h16m  tot: 0h7m45s  (33.4%)%  lr: 0.033524  loss: 0.002742  eta: 0h16m  tot: 0h7m45s  (33.4%)67.5%  lr: 0.033464  loss: 0.002742  eta: 0h16m  tot: 0h7m46s  (33.5%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.030180  loss: 0.002739  eta: 0h13m  tot: 0h9m10s  (40.0%)8.1%  lr: 0.033404  loss: 0.002742  eta: 0h16m  tot: 0h7m48s  (33.6%)68.2%  lr: 0.033393  loss: 0.002742  eta: 0h16m  tot: 0h7m48s  (33.6%)68.4%  lr: 0.033373  loss: 0.002740  eta: 0h16m  tot: 0h7m49s  (33.7%)68.9%  lr: 0.033323  loss: 0.002742  eta: 0h15m  tot: 0h7m50s  (33.8%)69.0%  lr: 0.033303  loss: 0.002742  eta: 0h15m  tot: 0h7m50s  (33.8%)71.1%  lr: 0.033083  loss: 0.002728  eta: 0h15m  tot: 0h7m55s  (34.2%)71.6%  lr: 0.033043  loss: 0.002728  eta: 0h15m  tot: 0h7m56s  (34.3%)71.8%  lr: 0.033023  loss: 0.002727  eta: 0h15m  tot: 0h7m57s  (34.4%)72.6%  lr: 0.032933  loss: 0.002731  eta: 0h15m  tot: 0h7m59s  (34.5%)73.6%  lr: 0.032833  loss: 0.002735  eta: 0h15m  tot: 0h8m1s  (34.7%)73.8%  lr: 0.032823  loss: 0.002733  eta: 0h15m  tot: 0h8m2s  (34.8%)75.1%  lr: 0.032683  loss: 0.002745  eta: 0h15m  tot: 0h8m5s  (35.0%)77.9%  lr: 0.032453  loss: 0.002741  eta: 0h15m  tot: 0h8m12s  (35.6%)78.5%  lr: 0.032393  loss: 0.002737  eta: 0h15m  tot: 0h8m13s  (35.7%)78.7%  lr: 0.032393  loss: 0.002737  eta: 0h15m  tot: 0h8m13s  (35.7%)78.8%  lr: 0.032352  loss: 0.002739  eta: 0h15m  tot: 0h8m14s  (35.8%)78.9%  lr: 0.032332  loss: 0.002739  eta: 0h15m  tot: 0h8m14s  (35.8%)79.0%  lr: 0.032302  loss: 0.002738  eta: 0h15m  tot: 0h8m14s  (35.8%)%  lr: 0.032302  loss: 0.002736  eta: 0h15m  tot: 0h8m15s  (35.8%)79.9%  lr: 0.032242  loss: 0.002731  eta: 0h15m  tot: 0h8m17s  (36.0%)80.7%  lr: 0.032142  loss: 0.002730  eta: 0h15m  tot: 0h8m19s  (36.1%)80.8%  lr: 0.032122  loss: 0.002735  eta: 0h15m  tot: 0h8m19s  (36.2%)81.2%  lr: 0.032102  loss: 0.002739  eta: 0h15m  tot: 0h8m20s  (36.2%)81.5%  lr: 0.032062  loss: 0.002739  eta: 0h15m  tot: 0h8m21s  (36.3%)81.8%  lr: 0.031972  loss: 0.002741  eta: 0h14m  tot: 0h8m22s  (36.4%)82.6%  lr: 0.031852  loss: 0.002737  eta: 0h14m  tot: 0h8m24s  (36.5%)82.8%  lr: 0.031822  loss: 0.002738  eta: 0h14m  tot: 0h8m25s  (36.6%)83.6%  lr: 0.031742  loss: 0.002735  eta: 0h14m  tot: 0h8m27s  (36.7%)86.0%  lr: 0.031582  loss: 0.002734  eta: 0h14m  tot: 0h8m33s  (37.2%)86.1%  lr: 0.031582  loss: 0.002733  eta: 0h14m  tot: 0h8m33s  (37.2%)86.3%  lr: 0.031552  loss: 0.002736  eta: 0h14m  tot: 0h8m34s  (37.3%)87.1%  lr: 0.031502  loss: 0.002730  eta: 0h14m  tot: 0h8m36s  (37.4%)87.4%  lr: 0.031482  loss: 0.002734  eta: 0h14m  tot: 0h8m37s  (37.5%)0.002732  eta: 0h14m  tot: 0h8m39s  (37.6%)89.4%  lr: 0.031311  loss: 0.002741  eta: 0h14m  tot: 0h8m42s  (37.9%)90.7%  lr: 0.031191  loss: 0.002743  eta: 0h14m  tot: 0h8m45s  (38.1%)%  lr: 0.031141  loss: 0.002742  eta: 0h14m  tot: 0h8m46s  (38.2%)91.1%  lr: 0.031141  loss: 0.002741  eta: 0h14m  tot: 0h8m46s  (38.2%)91.2%  lr: 0.031141  loss: 0.002740  eta: 0h14m  tot: 0h8m47s  (38.2%)m  tot: 0h8m48s  (38.4%)93.6%  lr: 0.030931  loss: 0.002736  eta: 0h14m  tot: 0h8m53s  (38.7%)93.9%  lr: 0.030901  loss: 0.002735  eta: 0h14m  tot: 0h8m53s  (38.8%)95.0%  lr: 0.030801  loss: 0.002734  eta: 0h14m  tot: 0h8m56s  (39.0%)95.8%  lr: 0.030761  loss: 0.002729  eta: 0h14m  tot: 0h8m58s  (39.2%)%  lr: 0.030721  loss: 0.002732  eta: 0h14m  tot: 0h8m59s  (39.2%)%  lr: 0.030671  loss: 0.002733  eta: 0h14m  tot: 0h9m0s  (39.3%)97.5%  lr: 0.030521  loss: 0.002736  eta: 0h14m  tot: 0h9m3s  (39.5%)97.6%  lr: 0.030501  loss: 0.002735  eta: 0h14m  tot: 0h9m3s  (39.5%)97.7%  lr: 0.030501  loss: 0.002735  eta: 0h14m  tot: 0h9m3s  (39.5%)99.1%  lr: 0.030330  loss: 0.002731  eta: 0h13m  tot: 0h9m7s  (39.8%)99.7%  lr: 0.030230  loss: 0.002733  eta: 0h13m  tot: 0h9m9s  (39.9%)99.7%  lr: 0.030200  loss: 0.002732  eta: 0h13m  tot: 0h9m9s  (39.9%)99.9%  lr: 0.030180  loss: 0.002737  eta: 0h13m  tot: 0h9m9s  (40.0%)\n",
      " ---+++                Epoch    1 Train error : 0.00269950 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n",
      "Epoch: 79.7%  lr: 0.022202  loss: 0.001865  eta: 0h9m  tot: 0h12m37s  (55.9%))%  lr: 0.029930  loss: 0.001322  eta: 0h9m  tot: 0h9m16s  (40.1%)1.1%  lr: 0.029880  loss: 0.001799  eta: 0h11m  tot: 0h9m17s  (40.2%)2.4%  lr: 0.029710  loss: 0.001642  eta: 0h12m  tot: 0h9m20s  (40.5%)2.5%  lr: 0.029710  loss: 0.001657  eta: 0h12m  tot: 0h9m21s  (40.5%)3.0%  lr: 0.029670  loss: 0.001659  eta: 0h12m  tot: 0h9m22s  (40.6%)3.1%  lr: 0.029640  loss: 0.001690  eta: 0h12m  tot: 0h9m22s  (40.6%)3.1%  lr: 0.029620  loss: 0.001678  eta: 0h12m  tot: 0h9m22s  (40.6%)3.5%  lr: 0.029560  loss: 0.001679  eta: 0h12m  tot: 0h9m23s  (40.7%)%  lr: 0.029540  loss: 0.001703  eta: 0h12m  tot: 0h9m24s  (40.7%)3.8%  lr: 0.029510  loss: 0.001679  eta: 0h12m  tot: 0h9m24s  (40.8%)4.0%  lr: 0.029489  loss: 0.001648  eta: 0h12m  tot: 0h9m24s  (40.8%)4.4%  lr: 0.029459  loss: 0.001615  eta: 0h12m  tot: 0h9m25s  (40.9%)5.3%  lr: 0.029359  loss: 0.001670  eta: 0h12m  tot: 0h9m28s  (41.1%)6.3%  lr: 0.029259  loss: 0.001743  eta: 0h11m  tot: 0h9m30s  (41.3%)6.8%  lr: 0.029189  loss: 0.001698  eta: 0h11m  tot: 0h9m31s  (41.4%)6.9%  lr: 0.029179  loss: 0.001701  eta: 0h11m  tot: 0h9m31s  (41.4%)7.0%  lr: 0.029169  loss: 0.001708  eta: 0h11m  tot: 0h9m32s  (41.4%)7.2%  lr: 0.029169  loss: 0.001699  eta: 0h12m  tot: 0h9m32s  (41.4%)7.3%  lr: 0.029159  loss: 0.001706  eta: 0h11m  tot: 0h9m32s  (41.5%)7.7%  lr: 0.029129  loss: 0.001683  eta: 0h12m  tot: 0h9m33s  (41.5%)8.1%  lr: 0.029119  loss: 0.001678  eta: 0h11m  tot: 0h9m34s  (41.6%)8.7%  lr: 0.029069  loss: 0.001684  eta: 0h11m  tot: 0h9m36s  (41.7%)9.2%  lr: 0.029039  loss: 0.001674  eta: 0h11m  tot: 0h9m37s  (41.8%)10.5%  lr: 0.028919  loss: 0.001690  eta: 0h11m  tot: 0h9m40s  (42.1%)10.6%  lr: 0.028919  loss: 0.001699  eta: 0h11m  tot: 0h9m40s  (42.1%)10.9%  lr: 0.028919  loss: 0.001711  eta: 0h11m  tot: 0h9m41s  (42.2%)11.5%  lr: 0.028859  loss: 0.001730  eta: 0h11m  tot: 0h9m42s  (42.3%)11.6%  lr: 0.028859  loss: 0.001721  eta: 0h11m  tot: 0h9m42s  (42.3%)13.6%  lr: 0.028639  loss: 0.001731  eta: 0h11m  tot: 0h9m47s  (42.7%)14.4%  lr: 0.028569  loss: 0.001746  eta: 0h11m  tot: 0h9m50s  (42.9%)14.9%  lr: 0.028549  loss: 0.001750  eta: 0h11m  tot: 0h9m51s  (43.0%)15.5%  lr: 0.028529  loss: 0.001764  eta: 0h11m  tot: 0h9m52s  (43.1%)15.7%  lr: 0.028509  loss: 0.001764  eta: 0h11m  tot: 0h9m53s  (43.1%)16.7%  lr: 0.028408  loss: 0.001754  eta: 0h11m  tot: 0h9m55s  (43.3%)18.3%  lr: 0.028268  loss: 0.001741  eta: 0h11m  tot: 0h9m59s  (43.7%)20.0%  lr: 0.028168  loss: 0.001765  eta: 0h11m  tot: 0h10m3s  (44.0%)21.4%  lr: 0.028058  loss: 0.001779  eta: 0h11m  tot: 0h10m7s  (44.3%)22.6%  lr: 0.027918  loss: 0.001782  eta: 0h11m  tot: 0h10m10s  (44.5%)23.1%  lr: 0.027828  loss: 0.001769  eta: 0h11m  tot: 0h10m11s  (44.6%)23.5%  lr: 0.027798  loss: 0.001773  eta: 0h11m  tot: 0h10m12s  (44.7%)24.8%  lr: 0.027618  loss: 0.001789  eta: 0h11m  tot: 0h10m16s  (45.0%)25.1%  lr: 0.027588  loss: 0.001795  eta: 0h11m  tot: 0h10m16s  (45.0%)h10m22s  (45.5%)28.2%  lr: 0.027307  loss: 0.001828  eta: 0h11m  tot: 0h10m24s  (45.6%)%  lr: 0.027287  loss: 0.001830  eta: 0h11m  tot: 0h10m25s  (45.7%)28.7%  lr: 0.027237  loss: 0.001823  eta: 0h11m  tot: 0h10m26s  (45.7%)29.9%  lr: 0.027127  loss: 0.001841  eta: 0h11m  tot: 0h10m29s  (46.0%)30.4%  lr: 0.027077  loss: 0.001842  eta: 0h11m  tot: 0h10m30s  (46.1%)30.6%  lr: 0.027067  loss: 0.001836  eta: 0h11m  tot: 0h10m30s  (46.1%)0h11m  tot: 0h10m31s  (46.1%)32.9%  lr: 0.026837  loss: 0.001810  eta: 0h11m  tot: 0h10m37s  (46.6%)33.4%  lr: 0.026807  loss: 0.001816  eta: 0h11m  tot: 0h10m38s  (46.7%)11m  tot: 0h10m38s  (46.7%)34.0%  lr: 0.026777  loss: 0.001821  eta: 0h11m  tot: 0h10m39s  (46.8%)34.7%  lr: 0.026707  loss: 0.001825  eta: 0h11m  tot: 0h10m41s  (46.9%)36.4%  lr: 0.026597  loss: 0.001837  eta: 0h11m  tot: 0h10m46s  (47.3%)36.5%  lr: 0.026577  loss: 0.001836  eta: 0h11m  tot: 0h10m46s  (47.3%)  tot: 0h10m46s  (47.3%)37.4%  lr: 0.026547  loss: 0.001844  eta: 0h10m  tot: 0h10m48s  (47.5%)%  lr: 0.026497  loss: 0.001848  eta: 0h11m  tot: 0h10m51s  (47.7%)41.5%  lr: 0.026246  loss: 0.001844  eta: 0h10m  tot: 0h11m0s  (48.3%)41.9%  lr: 0.026206  loss: 0.001852  eta: 0h10m  tot: 0h11m1s  (48.4%)42.1%  lr: 0.026156  loss: 0.001856  eta: 0h10m  tot: 0h11m1s  (48.4%)42.4%  lr: 0.026106  loss: 0.001856  eta: 0h10m  tot: 0h11m2s  (48.5%)0h10m  tot: 0h11m4s  (48.6%)43.2%  lr: 0.026026  loss: 0.001859  eta: 0h10m  tot: 0h11m4s  (48.6%)43.4%  lr: 0.025996  loss: 0.001857  eta: 0h10m  tot: 0h11m4s  (48.7%)44.0%  lr: 0.025896  loss: 0.001860  eta: 0h10m  tot: 0h11m6s  (48.8%)44.1%  lr: 0.025886  loss: 0.001858  eta: 0h10m  tot: 0h11m6s  (48.8%)44.2%  lr: 0.025866  loss: 0.001858  eta: 0h10m  tot: 0h11m6s  (48.8%)h11m10s  (49.1%)46.2%  lr: 0.025666  loss: 0.001862  eta: 0h10m  tot: 0h11m12s  (49.2%)46.5%  lr: 0.025636  loss: 0.001858  eta: 0h10m  tot: 0h11m12s  (49.3%)47.4%  lr: 0.025516  loss: 0.001855  eta: 0h10m  tot: 0h11m15s  (49.5%)48.5%  lr: 0.025395  loss: 0.001857  eta: 0h10m  tot: 0h11m18s  (49.7%)48.9%  lr: 0.025345  loss: 0.001857  eta: 0h10m  tot: 0h11m18s  (49.8%)49.4%  lr: 0.025285  loss: 0.001860  eta: 0h10m  tot: 0h11m20s  (49.9%)51.2%  lr: 0.025085  loss: 0.001870  eta: 0h10m  tot: 0h11m25s  (50.2%)51.4%  lr: 0.025065  loss: 0.001871  eta: 0h10m  tot: 0h11m25s  (50.3%)52.2%  lr: 0.024995  loss: 0.001869  eta: 0h10m  tot: 0h11m27s  (50.4%)52.7%  lr: 0.024935  loss: 0.001870  eta: 0h10m  tot: 0h11m28s  (50.5%)53.4%  lr: 0.024825  loss: 0.001866  eta: 0h10m  tot: 0h11m29s  (50.7%)53.6%  lr: 0.024805  loss: 0.001867  eta: 0h10m  tot: 0h11m30s  (50.7%)53.7%  lr: 0.024795  loss: 0.001866  eta: 0h10m  tot: 0h11m30s  (50.7%)54.6%  lr: 0.024715  loss: 0.001863  eta: 0h10m  tot: 0h11m33s  (50.9%)54.9%  lr: 0.024685  loss: 0.001866  eta: 0h10m  tot: 0h11m33s  (51.0%)55.6%  lr: 0.024645  loss: 0.001863  eta: 0h10m  tot: 0h11m35s  (51.1%)%  lr: 0.024545  loss: 0.001866  eta: 0h10m  tot: 0h11m38s  (51.3%)57.7%  lr: 0.024445  loss: 0.001869  eta: 0h10m  tot: 0h11m41s  (51.5%)58.2%  lr: 0.024334  loss: 0.001869  eta: 0h10m  tot: 0h11m42s  (51.6%)58.5%  lr: 0.024304  loss: 0.001874  eta: 0h10m  tot: 0h11m43s  (51.7%)58.6%  lr: 0.024304  loss: 0.001873  eta: 0h10m  tot: 0h11m43s  (51.7%)58.8%  lr: 0.024304  loss: 0.001874  eta: 0h10m  tot: 0h11m44s  (51.8%)60.1%  lr: 0.024184  loss: 0.001871  eta: 0h10m  tot: 0h11m47s  (52.0%)61.5%  lr: 0.023964  loss: 0.001866  eta: 0h10m  tot: 0h11m51s  (52.3%)61.9%  lr: 0.023904  loss: 0.001870  eta: 0h10m  tot: 0h11m52s  (52.4%)62.0%  lr: 0.023904  loss: 0.001870  eta: 0h10m  tot: 0h11m52s  (52.4%)63.3%  lr: 0.023824  loss: 0.001862  eta: 0h10m  tot: 0h11m55s  (52.7%)63.9%  lr: 0.023714  loss: 0.001862  eta: 0h10m  tot: 0h11m57s  (52.8%)64.1%  lr: 0.023694  loss: 0.001862  eta: 0h10m  tot: 0h11m58s  (52.8%)64.5%  lr: 0.023654  loss: 0.001862  eta: 0h10m  tot: 0h11m59s  (52.9%)%  lr: 0.023604  loss: 0.001858  eta: 0h9m  tot: 0h12m1s  (53.1%)67.1%  lr: 0.023404  loss: 0.001863  eta: 0h9m  tot: 0h12m5s  (53.4%)68.1%  lr: 0.023313  loss: 0.001867  eta: 0h9m  tot: 0h12m7s  (53.6%)68.9%  lr: 0.023253  loss: 0.001861  eta: 0h9m  tot: 0h12m9s  (53.8%)69.4%  lr: 0.023193  loss: 0.001860  eta: 0h9m  tot: 0h12m10s  (53.9%)69.6%  lr: 0.023133  loss: 0.001860  eta: 0h9m  tot: 0h12m11s  (53.9%)70.0%  lr: 0.023113  loss: 0.001859  eta: 0h9m  tot: 0h12m12s  (54.0%)72.6%  lr: 0.022803  loss: 0.001861  eta: 0h9m  tot: 0h12m19s  (54.5%)74.0%  lr: 0.022693  loss: 0.001855  eta: 0h9m  tot: 0h12m23s  (54.8%)74.1%  lr: 0.022693  loss: 0.001858  eta: 0h9m  tot: 0h12m23s  (54.8%)75.0%  lr: 0.022593  loss: 0.001860  eta: 0h9m  tot: 0h12m25s  (55.0%)75.1%  lr: 0.022583  loss: 0.001862  eta: 0h9m  tot: 0h12m26s  (55.0%)%  lr: 0.022523  loss: 0.001861  eta: 0h9m  tot: 0h12m27s  (55.2%)76.3%  lr: 0.022493  loss: 0.001857  eta: 0h9m  tot: 0h12m29s  (55.3%)76.4%  lr: 0.022493  loss: 0.001860  eta: 0h9m  tot: 0h12m29s  (55.3%)76.9%  lr: 0.022473  loss: 0.001858  eta: 0h9m  tot: 0h12m30s  (55.4%)77.5%  lr: 0.022463  loss: 0.001859  eta: 0h9m  tot: 0h12m32s  (55.5%)77.7%  lr: 0.022443  loss: 0.001860  eta: 0h9m  tot: 0h12m32s  (55.5%)0h12m34s  (55.7%)79.8%  lr: 0.022172  loss: 0.001864  eta: 0h9m  tot: 0h12m37s  (56.0%)Epoch: 100.0%  lr: 0.020260  loss: 0.001889  eta: 0h8m  tot: 0h13m30s  (60.0%)9.9%  lr: 0.022162  loss: 0.001865  eta: 0h9m  tot: 0h12m38s  (56.0%)80.9%  lr: 0.022092  loss: 0.001866  eta: 0h9m  tot: 0h12m41s  (56.2%)81.2%  lr: 0.022062  loss: 0.001864  eta: 0h9m  tot: 0h12m41s  (56.2%)81.5%  lr: 0.022042  loss: 0.001868  eta: 0h9m  tot: 0h12m42s  (56.3%)81.8%  lr: 0.022022  loss: 0.001867  eta: 0h9m  tot: 0h12m43s  (56.4%)82.6%  lr: 0.021932  loss: 0.001868  eta: 0h9m  tot: 0h12m45s  (56.5%)83.0%  lr: 0.021922  loss: 0.001866  eta: 0h9m  tot: 0h12m46s  (56.6%)83.2%  lr: 0.021902  loss: 0.001865  eta: 0h9m  tot: 0h12m46s  (56.6%)83.3%  lr: 0.021892  loss: 0.001865  eta: 0h9m  tot: 0h12m47s  (56.7%)84.1%  lr: 0.021792  loss: 0.001870  eta: 0h9m  tot: 0h12m49s  (56.8%)84.2%  lr: 0.021782  loss: 0.001872  eta: 0h9m  tot: 0h12m49s  (56.8%)85.1%  lr: 0.021682  loss: 0.001867  eta: 0h9m  tot: 0h12m51s  (57.0%)85.4%  lr: 0.021672  loss: 0.001869  eta: 0h9m  tot: 0h12m52s  (57.1%)86.0%  lr: 0.021662  loss: 0.001874  eta: 0h9m  tot: 0h12m53s  (57.2%)86.8%  lr: 0.021572  loss: 0.001874  eta: 0h9m  tot: 0h12m55s  (57.4%)89.6%  lr: 0.021361  loss: 0.001873  eta: 0h8m  tot: 0h13m3s  (57.9%)90.5%  lr: 0.021331  loss: 0.001877  eta: 0h8m  tot: 0h13m5s  (58.1%)91.1%  lr: 0.021271  loss: 0.001874  eta: 0h8m  tot: 0h13m7s  (58.2%)91.9%  lr: 0.021121  loss: 0.001872  eta: 0h8m  tot: 0h13m9s  (58.4%)92.9%  lr: 0.021011  loss: 0.001873  eta: 0h8m  tot: 0h13m11s  (58.6%)58.8%)97.9%  lr: 0.020431  loss: 0.001887  eta: 0h8m  tot: 0h13m24s  (59.6%)98.0%  lr: 0.020421  loss: 0.001887  eta: 0h8m  tot: 0h13m25s  (59.6%)98.1%  lr: 0.020411  loss: 0.001887  eta: 0h8m  tot: 0h13m25s  (59.6%)99.6%  lr: 0.020270  loss: 0.001888  eta: 0h8m  tot: 0h13m29s  (59.9%)\n",
      " ---+++                Epoch    2 Train error : 0.00189882 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70.8%  lr: 0.013123  loss: 0.001486  eta: 0h6m  tot: 0h16m54s  (74.2%).3%  lr: 0.020000  loss: 0.001663  eta: 0h3m  tot: 0h13m37s  (60.1%)0.4%  lr: 0.019990  loss: 0.001694  eta: 0h4m  tot: 0h13m37s  (60.1%)%  lr: 0.019920  loss: 0.001623  eta: 0h8m  tot: 0h13m39s  (60.2%)%  lr: 0.019740  loss: 0.001753  eta: 0h9m  tot: 0h13m44s  (60.6%)2.9%  lr: 0.019720  loss: 0.001721  eta: 0h9m  tot: 0h13m45s  (60.6%)%  lr: 0.019660  loss: 0.001673  eta: 0h9m  tot: 0h13m46s  (60.7%)3.6%  lr: 0.019630  loss: 0.001681  eta: 0h9m  tot: 0h13m47s  (60.7%)4.3%  lr: 0.019540  loss: 0.001615  eta: 0h9m  tot: 0h13m49s  (60.9%)4.7%  lr: 0.019520  loss: 0.001550  eta: 0h9m  tot: 0h13m50s  (60.9%)5.1%  lr: 0.019479  loss: 0.001564  eta: 0h9m  tot: 0h13m51s  (61.0%)5.8%  lr: 0.019419  loss: 0.001584  eta: 0h8m  tot: 0h13m53s  (61.2%)5.9%  lr: 0.019419  loss: 0.001569  eta: 0h9m  tot: 0h13m53s  (61.2%)6.4%  lr: 0.019349  loss: 0.001559  eta: 0h8m  tot: 0h13m54s  (61.3%)6.5%  lr: 0.019349  loss: 0.001575  eta: 0h8m  tot: 0h13m55s  (61.3%)%  lr: 0.019269  loss: 0.001564  eta: 0h8m  tot: 0h13m56s  (61.4%)7.2%  lr: 0.019259  loss: 0.001556  eta: 0h8m  tot: 0h13m56s  (61.4%)  loss: 0.001540  eta: 0h8m  tot: 0h13m57s  (61.5%)7.7%  lr: 0.019239  loss: 0.001514  eta: 0h8m  tot: 0h13m58s  (61.5%)7.7%  lr: 0.019229  loss: 0.001508  eta: 0h8m  tot: 0h13m58s  (61.5%)7.9%  lr: 0.019199  loss: 0.001495  eta: 0h8m  tot: 0h13m58s  (61.6%)8.3%  lr: 0.019159  loss: 0.001516  eta: 0h8m  tot: 0h14m0s  (61.7%)8.4%  lr: 0.019159  loss: 0.001527  eta: 0h8m  tot: 0h14m0s  (61.7%)8.5%  lr: 0.019149  loss: 0.001520  eta: 0h8m  tot: 0h14m0s  (61.7%)9.2%  lr: 0.019089  loss: 0.001487  eta: 0h8m  tot: 0h14m2s  (61.8%)9.7%  lr: 0.019049  loss: 0.001483  eta: 0h8m  tot: 0h14m3s  (61.9%)11.9%  lr: 0.018799  loss: 0.001486  eta: 0h8m  tot: 0h14m9s  (62.4%)12.0%  lr: 0.018779  loss: 0.001486  eta: 0h8m  tot: 0h14m9s  (62.4%)12.3%  lr: 0.018739  loss: 0.001468  eta: 0h8m  tot: 0h14m10s  (62.5%)12.7%  lr: 0.018679  loss: 0.001448  eta: 0h8m  tot: 0h14m11s  (62.5%)12.9%  lr: 0.018649  loss: 0.001440  eta: 0h8m  tot: 0h14m11s  (62.6%)13.5%  lr: 0.018609  loss: 0.001449  eta: 0h8m  tot: 0h14m13s  (62.7%)14.2%  lr: 0.018569  loss: 0.001472  eta: 0h8m  tot: 0h14m15s  (62.8%)%)15.9%  lr: 0.018468  loss: 0.001453  eta: 0h8m  tot: 0h14m19s  (63.2%)16.1%  lr: 0.018448  loss: 0.001450  eta: 0h8m  tot: 0h14m20s  (63.2%)16.3%  lr: 0.018418  loss: 0.001452  eta: 0h8m  tot: 0h14m20s  (63.3%)16.4%  lr: 0.018408  loss: 0.001449  eta: 0h8m  tot: 0h14m20s  (63.3%)16.8%  lr: 0.018378  loss: 0.001455  eta: 0h8m  tot: 0h14m22s  (63.4%)19.0%  lr: 0.018178  loss: 0.001470  eta: 0h8m  tot: 0h14m27s  (63.8%)28s  (63.8%)19.3%  lr: 0.018148  loss: 0.001468  eta: 0h8m  tot: 0h14m28s  (63.9%)19.6%  lr: 0.018128  loss: 0.001467  eta: 0h7m  tot: 0h14m29s  (63.9%)  eta: 0h7m  tot: 0h14m32s  (64.1%)20.8%  lr: 0.018018  loss: 0.001471  eta: 0h7m  tot: 0h14m32s  (64.2%)21.2%  lr: 0.017948  loss: 0.001482  eta: 0h7m  tot: 0h14m33s  (64.2%)%  lr: 0.017888  loss: 0.001486  eta: 0h7m  tot: 0h14m34s  (64.3%)22.1%  lr: 0.017858  loss: 0.001487  eta: 0h7m  tot: 0h14m35s  (64.4%)22.8%  lr: 0.017778  loss: 0.001495  eta: 0h7m  tot: 0h14m37s  (64.6%)23.6%  lr: 0.017688  loss: 0.001494  eta: 0h7m  tot: 0h14m39s  (64.7%)24.1%  lr: 0.017638  loss: 0.001486  eta: 0h7m  tot: 0h14m41s  (64.8%)0h7m  tot: 0h14m41s  (64.9%)  eta: 0h7m  tot: 0h14m42s  (64.9%)0.001485  eta: 0h7m  tot: 0h14m42s  (64.9%)25.6%  lr: 0.017548  loss: 0.001482  eta: 0h7m  tot: 0h14m44s  (65.1%)26.1%  lr: 0.017468  loss: 0.001484  eta: 0h7m  tot: 0h14m46s  (65.2%)26.2%  lr: 0.017468  loss: 0.001486  eta: 0h7m  tot: 0h14m46s  (65.2%)26.5%  lr: 0.017437  loss: 0.001493  eta: 0h7m  tot: 0h14m47s  (65.3%)27.8%  lr: 0.017297  loss: 0.001490  eta: 0h7m  tot: 0h14m51s  (65.6%)28.4%  lr: 0.017277  loss: 0.001486  eta: 0h7m  tot: 0h14m52s  (65.7%)29.5%  lr: 0.017147  loss: 0.001470  eta: 0h7m  tot: 0h14m55s  (65.9%)%  lr: 0.017017  loss: 0.001470  eta: 0h7m  tot: 0h14m58s  (66.1%)31.1%  lr: 0.016917  loss: 0.001466  eta: 0h7m  tot: 0h15m0s  (66.2%)31.2%  lr: 0.016917  loss: 0.001464  eta: 0h7m  tot: 0h15m0s  (66.2%)32.6%  lr: 0.016787  loss: 0.001458  eta: 0h7m  tot: 0h15m3s  (66.5%)33.8%  lr: 0.016637  loss: 0.001457  eta: 0h7m  tot: 0h15m6s  (66.8%)h7m  tot: 0h15m7s  (66.8%)34.2%  lr: 0.016587  loss: 0.001454  eta: 0h7m  tot: 0h15m7s  (66.8%)7m  tot: 0h15m8s  (66.9%)35.1%  lr: 0.016507  loss: 0.001456  eta: 0h7m  tot: 0h15m10s  (67.0%)35.3%  lr: 0.016497  loss: 0.001459  eta: 0h7m  tot: 0h15m10s  (67.1%)36.5%  lr: 0.016406  loss: 0.001462  eta: 0h7m  tot: 0h15m13s  (67.3%)37.9%  lr: 0.016286  loss: 0.001460  eta: 0h7m  tot: 0h15m16s  (67.6%)38.3%  lr: 0.016206  loss: 0.001464  eta: 0h7m  tot: 0h15m17s  (67.7%)39.2%  lr: 0.016156  loss: 0.001459  eta: 0h7m  tot: 0h15m19s  (67.8%)40.3%  lr: 0.015996  loss: 0.001459  eta: 0h6m  tot: 0h15m22s  (68.1%)41.8%  lr: 0.015796  loss: 0.001476  eta: 0h6m  tot: 0h15m26s  (68.4%)42.5%  lr: 0.015746  loss: 0.001476  eta: 0h6m  tot: 0h15m28s  (68.5%)43.0%  lr: 0.015626  loss: 0.001486  eta: 0h6m  tot: 0h15m29s  (68.6%)h6m  tot: 0h15m31s  (68.7%)43.8%  lr: 0.015566  loss: 0.001484  eta: 0h6m  tot: 0h15m31s  (68.8%)%  lr: 0.015476  loss: 0.001483  eta: 0h6m  tot: 0h15m35s  (69.0%)45.2%  lr: 0.015436  loss: 0.001483  eta: 0h6m  tot: 0h15m35s  (69.0%)45.5%  lr: 0.015405  loss: 0.001484  eta: 0h6m  tot: 0h15m36s  (69.1%)45.6%  lr: 0.015405  loss: 0.001484  eta: 0h6m  tot: 0h15m36s  (69.1%)45.9%  lr: 0.015375  loss: 0.001486  eta: 0h6m  tot: 0h15m37s  (69.2%)%  lr: 0.015365  loss: 0.001485  eta: 0h6m  tot: 0h15m38s  (69.2%)46.5%  lr: 0.015295  loss: 0.001485  eta: 0h6m  tot: 0h15m39s  (69.3%)47.2%  lr: 0.015225  loss: 0.001489  eta: 0h6m  tot: 0h15m40s  (69.4%)47.4%  lr: 0.015205  loss: 0.001488  eta: 0h6m  tot: 0h15m41s  (69.5%)48.3%  lr: 0.015145  loss: 0.001487  eta: 0h6m  tot: 0h15m43s  (69.7%)48.9%  lr: 0.015095  loss: 0.001488  eta: 0h6m  tot: 0h15m45s  (69.8%)%  lr: 0.015075  loss: 0.001485  eta: 0h6m  tot: 0h15m46s  (69.8%)49.2%  lr: 0.015035  loss: 0.001483  eta: 0h6m  tot: 0h15m46s  (69.8%)49.8%  lr: 0.014975  loss: 0.001480  eta: 0h6m  tot: 0h15m47s  (70.0%)51.0%  lr: 0.014895  loss: 0.001478  eta: 0h6m  tot: 0h15m50s  (70.2%)51.9%  lr: 0.014835  loss: 0.001481  eta: 0h6m  tot: 0h15m53s  (70.4%)52.0%  lr: 0.014835  loss: 0.001482  eta: 0h6m  tot: 0h15m53s  (70.4%)52.4%  lr: 0.014775  loss: 0.001480  eta: 0h6m  tot: 0h15m54s  (70.5%)53.8%  lr: 0.014655  loss: 0.001477  eta: 0h6m  tot: 0h15m58s  (70.8%)54.0%  lr: 0.014645  loss: 0.001478  eta: 0h6m  tot: 0h15m58s  (70.8%)54.3%  lr: 0.014605  loss: 0.001477  eta: 0h6m  tot: 0h15m59s  (70.9%)54.6%  lr: 0.014595  loss: 0.001475  eta: 0h6m  tot: 0h16m0s  (70.9%)%  lr: 0.014485  loss: 0.001477  eta: 0h6m  tot: 0h16m2s  (71.1%)55.9%  lr: 0.014455  loss: 0.001476  eta: 0h6m  tot: 0h16m3s  (71.2%)56.1%  lr: 0.014445  loss: 0.001476  eta: 0h6m  tot: 0h16m4s  (71.2%)56.2%  lr: 0.014435  loss: 0.001476  eta: 0h6m  tot: 0h16m4s  (71.2%)56.4%  lr: 0.014415  loss: 0.001476  eta: 0h6m  tot: 0h16m4s  (71.3%)59.0%  lr: 0.014214  loss: 0.001473  eta: 0h6m  tot: 0h16m11s  (71.8%)59.3%  lr: 0.014174  loss: 0.001471  eta: 0h6m  tot: 0h16m12s  (71.9%)59.7%  lr: 0.014144  loss: 0.001471  eta: 0h6m  tot: 0h16m13s  (71.9%)60.4%  lr: 0.014074  loss: 0.001470  eta: 0h6m  tot: 0h16m15s  (72.1%)60.5%  lr: 0.014074  loss: 0.001470  eta: 0h6m  tot: 0h16m16s  (72.1%)%  lr: 0.013954  loss: 0.001474  eta: 0h6m  tot: 0h16m20s  (72.4%)62.2%  lr: 0.013944  loss: 0.001476  eta: 0h6m  tot: 0h16m20s  (72.4%)62.8%  lr: 0.013874  loss: 0.001475  eta: 0h6m  tot: 0h16m22s  (72.6%)64.5%  lr: 0.013774  loss: 0.001479  eta: 0h5m  tot: 0h16m27s  (72.9%)64.7%  lr: 0.013754  loss: 0.001481  eta: 0h5m  tot: 0h16m27s  (72.9%)65.7%  lr: 0.013644  loss: 0.001480  eta: 0h5m  tot: 0h16m30s  (73.1%)66.7%  lr: 0.013534  loss: 0.001481  eta: 0h5m  tot: 0h16m33s  (73.3%)67.1%  lr: 0.013504  loss: 0.001480  eta: 0h6m  tot: 0h16m39s  (73.4%)69.0%  lr: 0.013373  loss: 0.001486  eta: 0h6m  tot: 0h16m49s  (73.8%)69.4%  lr: 0.013313  loss: 0.001485  eta: 0h6m  tot: 0h16m50s  (73.9%)70.4%  lr: 0.013173  loss: 0.001490  eta: 0h6m  tot: 0h16m53s  (74.1%)70.9%  lr: 0.013113  loss: 0.001486  eta: 0h5m  tot: 0h16m54s  (74.2%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.010140  loss: 0.001540  eta: 0h4m  tot: 0h18m23s  (80.0%)1.2%  lr: 0.013063  loss: 0.001488  eta: 0h5m  tot: 0h16m55s  (74.2%)73.5%  lr: 0.012843  loss: 0.001491  eta: 0h5m  tot: 0h17m2s  (74.7%)73.8%  lr: 0.012823  loss: 0.001494  eta: 0h5m  tot: 0h17m3s  (74.8%)0h5m  tot: 0h17m5s  (74.9%)75.5%  lr: 0.012613  loss: 0.001503  eta: 0h5m  tot: 0h17m8s  (75.1%)75.7%  lr: 0.012613  loss: 0.001505  eta: 0h5m  tot: 0h17m9s  (75.1%)76.2%  lr: 0.012583  loss: 0.001505  eta: 0h5m  tot: 0h17m10s  (75.2%)76.3%  lr: 0.012583  loss: 0.001504  eta: 0h5m  tot: 0h17m11s  (75.3%)s  (75.3%)0h5m  tot: 0h17m12s  (75.3%)76.9%  lr: 0.012543  loss: 0.001504  eta: 0h5m  tot: 0h17m12s  (75.4%)77.0%  lr: 0.012543  loss: 0.001504  eta: 0h5m  tot: 0h17m13s  (75.4%)77.2%  lr: 0.012503  loss: 0.001507  eta: 0h5m  tot: 0h17m13s  (75.4%)77.4%  lr: 0.012493  loss: 0.001506  eta: 0h5m  tot: 0h17m14s  (75.5%)77.5%  lr: 0.012493  loss: 0.001505  eta: 0h5m  tot: 0h17m14s  (75.5%)78.7%  lr: 0.012393  loss: 0.001508  eta: 0h5m  tot: 0h17m17s  (75.7%)78.8%  lr: 0.012382  loss: 0.001509  eta: 0h5m  tot: 0h17m17s  (75.8%)78.9%  lr: 0.012372  loss: 0.001510  eta: 0h5m  tot: 0h17m18s  (75.8%)80.1%  lr: 0.012192  loss: 0.001513  eta: 0h5m  tot: 0h17m24s  (76.0%)80.8%  lr: 0.012132  loss: 0.001513  eta: 0h5m  tot: 0h17m26s  (76.2%)81.1%  lr: 0.012112  loss: 0.001516  eta: 0h5m  tot: 0h17m27s  (76.2%)81.7%  lr: 0.012062  loss: 0.001514  eta: 0h5m  tot: 0h17m30s  (76.3%)%  lr: 0.012002  loss: 0.001515  eta: 0h5m  tot: 0h17m31s  (76.4%)82.9%  lr: 0.011902  loss: 0.001517  eta: 0h5m  tot: 0h17m34s  (76.6%)m35s  (76.7%)h17m35s  (76.7%)%  lr: 0.011632  loss: 0.001526  eta: 0h5m  tot: 0h17m41s  (77.1%)85.5%  lr: 0.011602  loss: 0.001528  eta: 0h5m  tot: 0h17m41s  (77.1%)  eta: 0h5m  tot: 0h17m43s  (77.2%)87.2%  lr: 0.011311  loss: 0.001529  eta: 0h5m  tot: 0h17m46s  (77.4%)88.3%  lr: 0.011161  loss: 0.001526  eta: 0h5m  tot: 0h17m49s  (77.7%)89.0%  lr: 0.011131  loss: 0.001525  eta: 0h5m  tot: 0h17m52s  (77.8%)89.9%  lr: 0.011071  loss: 0.001524  eta: 0h5m  tot: 0h17m55s  (78.0%)90.7%  lr: 0.010991  loss: 0.001527  eta: 0h5m  tot: 0h17m57s  (78.1%)90.9%  lr: 0.010981  loss: 0.001526  eta: 0h5m  tot: 0h17m58s  (78.2%)91.6%  lr: 0.010951  loss: 0.001528  eta: 0h5m  tot: 0h17m59s  (78.3%)91.8%  lr: 0.010921  loss: 0.001528  eta: 0h5m  tot: 0h18m0s  (78.4%)92.9%  lr: 0.010781  loss: 0.001530  eta: 0h5m  tot: 0h18m4s  (78.6%)93.9%  lr: 0.010701  loss: 0.001531  eta: 0h5m  tot: 0h18m6s  (78.8%)94.2%  lr: 0.010661  loss: 0.001531  eta: 0h5m  tot: 0h18m7s  (78.8%)94.8%  lr: 0.010591  loss: 0.001530  eta: 0h5m  tot: 0h18m8s  (79.0%)95.2%  lr: 0.010581  loss: 0.001532  eta: 0h5m  tot: 0h18m10s  (79.0%)95.5%  lr: 0.010531  loss: 0.001533  eta: 0h4m  tot: 0h18m11s  (79.1%)96.2%  lr: 0.010491  loss: 0.001533  eta: 0h4m  tot: 0h18m13s  (79.2%)96.3%  lr: 0.010471  loss: 0.001533  eta: 0h4m  tot: 0h18m13s  (79.3%)96.7%  lr: 0.010451  loss: 0.001532  eta: 0h4m  tot: 0h18m14s  (79.3%)97.5%  lr: 0.010411  loss: 0.001534  eta: 0h4m  tot: 0h18m16s  (79.5%)97.6%  lr: 0.010411  loss: 0.001536  eta: 0h4m  tot: 0h18m16s  (79.5%)98.7%  lr: 0.010290  loss: 0.001539  eta: 0h4m  tot: 0h18m19s  (79.7%)99.3%  lr: 0.010220  loss: 0.001537  eta: 0h4m  tot: 0h18m21s  (79.9%)99.7%  lr: 0.010190  loss: 0.001539  eta: 0h4m  tot: 0h18m22s  (79.9%)99.8%  lr: 0.010190  loss: 0.001539  eta: 0h4m  tot: 0h18m22s  (80.0%)\n",
      " ---+++                Epoch    3 Train error : 0.00154462 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n",
      "Epoch: 73.6%  lr: 0.002903  loss: 0.001352  eta: 0h1m  tot: 0h21m50s  (94.7%).8%  lr: 0.009950  loss: 0.001451  eta: 0h4m  tot: 0h18m30s  (80.2%)0.9%  lr: 0.009940  loss: 0.001426  eta: 0h4m  tot: 0h18m30s  (80.2%)2.4%  lr: 0.009720  loss: 0.001344  eta: 0h4m  tot: 0h18m34s  (80.5%)3.3%  lr: 0.009570  loss: 0.001397  eta: 0h4m  tot: 0h18m37s  (80.7%)3.5%  lr: 0.009520  loss: 0.001397  eta: 0h4m  tot: 0h18m37s  (80.7%)4.0%  lr: 0.009489  loss: 0.001407  eta: 0h4m  tot: 0h18m38s  (80.8%)4.1%  lr: 0.009459  loss: 0.001397  eta: 0h4m  tot: 0h18m39s  (80.8%)5.9%  lr: 0.009369  loss: 0.001382  eta: 0h3m  tot: 0h18m43s  (81.2%)6.0%  lr: 0.009359  loss: 0.001386  eta: 0h4m  tot: 0h18m44s  (81.2%)6.6%  lr: 0.009289  loss: 0.001381  eta: 0h3m  tot: 0h18m45s  (81.3%)8.3%  lr: 0.009109  loss: 0.001331  eta: 0h3m  tot: 0h18m49s  (81.7%)8.9%  lr: 0.009019  loss: 0.001372  eta: 0h3m  tot: 0h18m51s  (81.8%)9.4%  lr: 0.008999  loss: 0.001377  eta: 0h3m  tot: 0h18m52s  (81.9%)9.7%  lr: 0.008979  loss: 0.001372  eta: 0h3m  tot: 0h18m54s  (81.9%)9.8%  lr: 0.008959  loss: 0.001368  eta: 0h3m  tot: 0h18m54s  (82.0%)10.4%  lr: 0.008939  loss: 0.001363  eta: 0h3m  tot: 0h18m55s  (82.1%)11.0%  lr: 0.008889  loss: 0.001370  eta: 0h3m  tot: 0h18m57s  (82.2%)11.5%  lr: 0.008829  loss: 0.001389  eta: 0h3m  tot: 0h18m59s  (82.3%)12.1%  lr: 0.008779  loss: 0.001371  eta: 0h3m  tot: 0h19m0s  (82.4%)13.1%  lr: 0.008719  loss: 0.001371  eta: 0h3m  tot: 0h19m4s  (82.6%)13.7%  lr: 0.008649  loss: 0.001376  eta: 0h3m  tot: 0h19m5s  (82.7%)13.9%  lr: 0.008589  loss: 0.001375  eta: 0h3m  tot: 0h19m6s  (82.8%)15.7%  lr: 0.008428  loss: 0.001356  eta: 0h3m  tot: 0h19m11s  (83.1%)%  lr: 0.008268  loss: 0.001381  eta: 0h3m  tot: 0h19m14s  (83.3%)16.9%  lr: 0.008248  loss: 0.001382  eta: 0h3m  tot: 0h19m14s  (83.4%)18.1%  lr: 0.008098  loss: 0.001373  eta: 0h3m  tot: 0h19m17s  (83.6%)19.4%  lr: 0.007948  loss: 0.001379  eta: 0h3m  tot: 0h19m20s  (83.9%)19.6%  lr: 0.007928  loss: 0.001387  eta: 0h3m  tot: 0h19m21s  (83.9%)20.9%  lr: 0.007768  loss: 0.001390  eta: 0h3m  tot: 0h19m24s  (84.2%)21.0%  lr: 0.007758  loss: 0.001386  eta: 0h3m  tot: 0h19m24s  (84.2%)22.0%  lr: 0.007678  loss: 0.001388  eta: 0h3m  tot: 0h19m27s  (84.4%)22.4%  lr: 0.007668  loss: 0.001390  eta: 0h3m  tot: 0h19m28s  (84.5%)22.8%  lr: 0.007638  loss: 0.001387  eta: 0h3m  tot: 0h19m29s  (84.6%)23.0%  lr: 0.007618  loss: 0.001387  eta: 0h3m  tot: 0h19m30s  (84.6%)23.1%  lr: 0.007618  loss: 0.001387  eta: 0h3m  tot: 0h19m30s  (84.6%)23.9%  lr: 0.007568  loss: 0.001392  eta: 0h3m  tot: 0h19m32s  (84.8%)24.6%  lr: 0.007498  loss: 0.001393  eta: 0h3m  tot: 0h19m34s  (84.9%)25.4%  lr: 0.007407  loss: 0.001401  eta: 0h3m  tot: 0h19m36s  (85.1%)25.5%  lr: 0.007397  loss: 0.001402  eta: 0h3m  tot: 0h19m36s  (85.1%)26.3%  lr: 0.007337  loss: 0.001395  eta: 0h3m  tot: 0h19m38s  (85.3%)26.7%  lr: 0.007337  loss: 0.001401  eta: 0h3m  tot: 0h19m39s  (85.3%)26.9%  lr: 0.007337  loss: 0.001399  eta: 0h3m  tot: 0h19m40s  (85.4%)27.0%  lr: 0.007337  loss: 0.001399  eta: 0h3m  tot: 0h19m40s  (85.4%)27.2%  lr: 0.007327  loss: 0.001400  eta: 0h3m  tot: 0h19m40s  (85.4%)28.4%  lr: 0.007237  loss: 0.001401  eta: 0h3m  tot: 0h19m43s  (85.7%)0.001396  eta: 0h3m  tot: 0h19m44s  (85.7%)29.4%  lr: 0.007097  loss: 0.001395  eta: 0h3m  tot: 0h19m46s  (85.9%)31.8%  lr: 0.006907  loss: 0.001369  eta: 0h3m  tot: 0h19m52s  (86.4%)32.0%  lr: 0.006857  loss: 0.001368  eta: 0h2m  tot: 0h19m53s  (86.4%)32.1%  lr: 0.006847  loss: 0.001365  eta: 0h2m  tot: 0h19m53s  (86.4%)32.2%  lr: 0.006837  loss: 0.001371  eta: 0h2m  tot: 0h19m53s  (86.4%)32.3%  lr: 0.006807  loss: 0.001370  eta: 0h2m  tot: 0h19m54s  (86.5%)32.6%  lr: 0.006807  loss: 0.001370  eta: 0h2m  tot: 0h19m54s  (86.5%)33.6%  lr: 0.006657  loss: 0.001360  eta: 0h2m  tot: 0h19m57s  (86.7%)34.1%  lr: 0.006567  loss: 0.001355  eta: 0h2m  tot: 0h19m59s  (86.8%)34.2%  lr: 0.006537  loss: 0.001352  eta: 0h2m  tot: 0h19m59s  (86.8%)34.3%  lr: 0.006527  loss: 0.001351  eta: 0h2m  tot: 0h19m59s  (86.9%)35.6%  lr: 0.006406  loss: 0.001368  eta: 0h2m  tot: 0h20m2s  (87.1%)  lr: 0.006286  loss: 0.001365  eta: 0h2m  tot: 0h20m6s  (87.4%)38.4%  lr: 0.006166  loss: 0.001355  eta: 0h2m  tot: 0h20m11s  (87.7%)39.5%  lr: 0.006086  loss: 0.001357  eta: 0h2m  tot: 0h20m14s  (87.9%)39.8%  lr: 0.006066  loss: 0.001357  eta: 0h2m  tot: 0h20m15s  (88.0%)h2m  tot: 0h20m17s  (88.1%)41.1%  lr: 0.005946  loss: 0.001346  eta: 0h2m  tot: 0h20m20s  (88.2%)%  lr: 0.005906  loss: 0.001346  eta: 0h2m  tot: 0h20m22s  (88.4%)%  lr: 0.005846  loss: 0.001341  eta: 0h2m  tot: 0h20m23s  (88.4%)42.4%  lr: 0.005796  loss: 0.001344  eta: 0h2m  tot: 0h20m24s  (88.5%)42.6%  lr: 0.005786  loss: 0.001344  eta: 0h2m  tot: 0h20m24s  (88.5%)42.8%  lr: 0.005746  loss: 0.001342  eta: 0h2m  tot: 0h20m25s  (88.6%)44.1%  lr: 0.005606  loss: 0.001339  eta: 0h2m  tot: 0h20m29s  (88.8%)44.3%  lr: 0.005566  loss: 0.001336  eta: 0h2m  tot: 0h20m30s  (88.9%)44.7%  lr: 0.005536  loss: 0.001336  eta: 0h2m  tot: 0h20m31s  (88.9%)44.8%  lr: 0.005526  loss: 0.001335  eta: 0h2m  tot: 0h20m31s  (89.0%)45.3%  lr: 0.005526  loss: 0.001329  eta: 0h2m  tot: 0h20m32s  (89.1%)48.2%  lr: 0.005245  loss: 0.001321  eta: 0h2m  tot: 0h20m40s  (89.6%)%  lr: 0.005205  loss: 0.001327  eta: 0h2m  tot: 0h20m41s  (89.8%)48.9%  lr: 0.005205  loss: 0.001327  eta: 0h2m  tot: 0h20m42s  (89.8%)0.001327  eta: 0h2m  tot: 0h20m43s  (89.9%)49.8%  lr: 0.005125  loss: 0.001326  eta: 0h2m  tot: 0h20m44s  (90.0%)50.8%  lr: 0.005055  loss: 0.001326  eta: 0h2m  tot: 0h20m46s  (90.2%)51.9%  lr: 0.004935  loss: 0.001335  eta: 0h2m  tot: 0h20m51s  (90.4%)52.1%  lr: 0.004915  loss: 0.001333  eta: 0h2m  tot: 0h20m51s  (90.4%)52.8%  lr: 0.004875  loss: 0.001335  eta: 0h2m  tot: 0h20m54s  (90.6%)53.7%  lr: 0.004775  loss: 0.001338  eta: 0h2m  tot: 0h20m57s  (90.7%)%  lr: 0.004755  loss: 0.001337  eta: 0h2m  tot: 0h20m57s  (90.8%)54.3%  lr: 0.004685  loss: 0.001339  eta: 0h2m  tot: 0h20m58s  (90.9%)55.0%  lr: 0.004625  loss: 0.001341  eta: 0h2m  tot: 0h21m0s  (91.0%)55.2%  lr: 0.004595  loss: 0.001342  eta: 0h2m  tot: 0h21m1s  (91.0%)55.4%  lr: 0.004585  loss: 0.001342  eta: 0h2m  tot: 0h21m1s  (91.1%)56.2%  lr: 0.004465  loss: 0.001349  eta: 0h2m  tot: 0h21m3s  (91.2%)56.8%  lr: 0.004404  loss: 0.001351  eta: 0h1m  tot: 0h21m5s  (91.4%)57.2%  lr: 0.004354  loss: 0.001353  eta: 0h1m  tot: 0h21m6s  (91.4%)57.6%  lr: 0.004314  loss: 0.001353  eta: 0h1m  tot: 0h21m7s  (91.5%)57.9%  lr: 0.004304  loss: 0.001353  eta: 0h1m  tot: 0h21m8s  (91.6%)58.6%  lr: 0.004234  loss: 0.001349  eta: 0h1m  tot: 0h21m10s  (91.7%)0.004074  loss: 0.001351  eta: 0h1m  tot: 0h21m13s  (92.0%)60.9%  lr: 0.003984  loss: 0.001349  eta: 0h1m  tot: 0h21m15s  (92.2%)61.1%  lr: 0.003974  loss: 0.001348  eta: 0h1m  tot: 0h21m16s  (92.2%)62.7%  lr: 0.003834  loss: 0.001347  eta: 0h1m  tot: 0h21m20s  (92.5%)62.7%  lr: 0.003834  loss: 0.001346  eta: 0h1m  tot: 0h21m20s  (92.5%)63.5%  lr: 0.003754  loss: 0.001347  eta: 0h1m  tot: 0h21m22s  (92.7%)63.9%  lr: 0.003694  loss: 0.001349  eta: 0h1m  tot: 0h21m23s  (92.8%)64.3%  lr: 0.003644  loss: 0.001351  eta: 0h1m  tot: 0h21m24s  (92.9%)64.5%  lr: 0.003634  loss: 0.001351  eta: 0h1m  tot: 0h21m25s  (92.9%)65.0%  lr: 0.003624  loss: 0.001355  eta: 0h1m  tot: 0h21m26s  (93.0%)65.3%  lr: 0.003604  loss: 0.001352  eta: 0h1m  tot: 0h21m27s  (93.1%)67.0%  lr: 0.003514  loss: 0.001355  eta: 0h1m  tot: 0h21m31s  (93.4%)68.0%  lr: 0.003444  loss: 0.001352  eta: 0h1m  tot: 0h21m33s  (93.6%)68.4%  lr: 0.003403  loss: 0.001358  eta: 0h1m  tot: 0h21m35s  (93.7%)68.9%  lr: 0.003383  loss: 0.001355  eta: 0h1m  tot: 0h21m36s  (93.8%)69.2%  lr: 0.003323  loss: 0.001352  eta: 0h1m  tot: 0h21m37s  (93.8%)70.4%  lr: 0.003163  loss: 0.001352  eta: 0h1m  tot: 0h21m42s  (94.1%)70.5%  lr: 0.003153  loss: 0.001352  eta: 0h1m  tot: 0h21m42s  (94.1%)70.8%  lr: 0.003113  loss: 0.001354  eta: 0h1m  tot: 0h21m43s  (94.2%)71.3%  lr: 0.003083  loss: 0.001357  eta: 0h1m  tot: 0h21m44s  (94.3%)71.7%  lr: 0.003073  loss: 0.001356  eta: 0h1m  tot: 0h21m45s  (94.3%)71.8%  lr: 0.003073  loss: 0.001355  eta: 0h1m  tot: 0h21m46s  (94.4%)72.0%  lr: 0.003043  loss: 0.001354  eta: 0h1m  tot: 0h21m46s  (94.4%)73.7%  lr: 0.002903  loss: 0.001351  eta: 0h1m  tot: 0h21m51s  (94.7%)Epoch: 99.9%  lr: 0.000260  loss: 0.001350  eta: <1min   tot: 0h23m3s  (100.0%)s  (94.8%)75.5%  lr: 0.002653  loss: 0.001355  eta: 0h1m  tot: 0h21m56s  (95.1%)76.5%  lr: 0.002583  loss: 0.001354  eta: 0h1m  tot: 0h21m58s  (95.3%)76.8%  lr: 0.002533  loss: 0.001352  eta: 0h1m  tot: 0h21m59s  (95.4%)78.0%  lr: 0.002423  loss: 0.001349  eta: 0h1m  tot: 0h22m3s  (95.6%)78.9%  lr: 0.002332  loss: 0.001348  eta: <1min   tot: 0h22m5s  (95.8%)79.9%  lr: 0.002192  loss: 0.001349  eta: <1min   tot: 0h22m8s  (96.0%)80.1%  lr: 0.002182  loss: 0.001350  eta: <1min   tot: 0h22m8s  (96.0%)0.001353  eta: <1min   tot: 0h22m10s  (96.2%)80.9%  lr: 0.002082  loss: 0.001354  eta: <1min   tot: 0h22m11s  (96.2%)81.8%  lr: 0.001982  loss: 0.001351  eta: <1min   tot: 0h22m13s  (96.4%)83.0%  lr: 0.001872  loss: 0.001350  eta: <1min   tot: 0h22m16s  (96.6%)83.5%  lr: 0.001782  loss: 0.001346  eta: <1min   tot: 0h22m18s  (96.7%)83.7%  lr: 0.001732  loss: 0.001346  eta: <1min   tot: 0h22m18s  (96.7%)84.4%  lr: 0.001652  loss: 0.001344  eta: <1min   tot: 0h22m20s  (96.9%)84.5%  lr: 0.001652  loss: 0.001344  eta: <1min   tot: 0h22m21s  (96.9%)84.7%  lr: 0.001622  loss: 0.001343  eta: <1min   tot: 0h22m21s  (96.9%)%  lr: 0.001612  loss: 0.001344  eta: <1min   tot: 0h22m21s  (97.0%)85.2%  lr: 0.001582  loss: 0.001344  eta: <1min   tot: 0h22m22s  (97.0%)85.5%  lr: 0.001532  loss: 0.001343  eta: <1min   tot: 0h22m23s  (97.1%)85.7%  lr: 0.001512  loss: 0.001345  eta: <1min   tot: 0h22m24s  (97.1%)86.2%  lr: 0.001502  loss: 0.001343  eta: <1min   tot: 0h22m25s  (97.2%)86.2%  lr: 0.001492  loss: 0.001343  eta: <1min   tot: 0h22m25s  (97.2%)86.3%  lr: 0.001482  loss: 0.001342  eta: <1min   tot: 0h22m25s  (97.3%)86.8%  lr: 0.001422  loss: 0.001342  eta: <1min   tot: 0h22m27s  (97.4%)86.9%  lr: 0.001411  loss: 0.001342  eta: <1min   tot: 0h22m27s  (97.4%)87.2%  lr: 0.001391  loss: 0.001342  eta: <1min   tot: 0h22m28s  (97.4%)88.5%  lr: 0.001281  loss: 0.001345  eta: <1min   tot: 0h22m31s  (97.7%)88.9%  lr: 0.001231  loss: 0.001345  eta: <1min   tot: 0h22m32s  (97.8%)89.8%  lr: 0.001181  loss: 0.001346  eta: <1min   tot: 0h22m34s  (98.0%)90.1%  lr: 0.001161  loss: 0.001346  eta: <1min   tot: 0h22m35s  (98.0%)91.1%  lr: 0.001031  loss: 0.001345  eta: <1min   tot: 0h22m38s  (98.2%)91.9%  lr: 0.000961  loss: 0.001349  eta: <1min   tot: 0h22m40s  (98.4%)92.5%  lr: 0.000901  loss: 0.001351  eta: <1min   tot: 0h22m41s  (98.5%)93.2%  lr: 0.000851  loss: 0.001351  eta: <1min   tot: 0h22m43s  (98.6%)93.5%  lr: 0.000811  loss: 0.001351  eta: <1min   tot: 0h22m44s  (98.7%)94.2%  lr: 0.000751  loss: 0.001349  eta: <1min   tot: 0h22m46s  (98.8%)  lr: 0.000721  loss: 0.001350  eta: <1min   tot: 0h22m46s  (98.9%)94.8%  lr: 0.000691  loss: 0.001350  eta: <1min   tot: 0h22m48s  (99.0%)96.1%  lr: 0.000561  loss: 0.001350  eta: <1min   tot: 0h22m52s  (99.2%)97.0%  lr: 0.000501  loss: 0.001351  eta: <1min   tot: 0h22m54s  (99.4%)97.3%  lr: 0.000481  loss: 0.001351  eta: <1min   tot: 0h22m55s  (99.5%)98.5%  lr: 0.000370  loss: 0.001350  eta: <1min   tot: 0h22m59s  (99.7%)98.6%  lr: 0.000370  loss: 0.001350  eta: <1min   tot: 0h22m59s  (99.7%)99.0%  lr: 0.000350  loss: 0.001350  eta: <1min   tot: 0h23m0s  (99.8%)99.6%  lr: 0.000300  loss: 0.001348  eta: <1min   tot: 0h23m2s  (99.9%)99.7%  lr: 0.000300  loss: 0.001349  eta: <1min   tot: 0h23m2s  (99.9%)100.0%  lr: 0.000250  loss: 0.001350  eta: <1min   tot: 0h23m3s  (100.0%)\n",
      " ---+++                Epoch    4 Train error : 0.00137647 +++--- ���\n",
      "Saving model to file : data/model.tsv\n",
      "Saving model in tsv format : data/model.tsv.tsv\n"
     ]
    }
   ],
   "source": [
    "!starspace train -trainFile data/train_prep.tsv -model data/qmodel -trainMode 3 -fileFormat labelDoc \\\n",
    "    -ngrams 1 -epoch 5 -dim 100 -similarity cosine -minCount 2 -negSearchLimit 10 -lr 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings = { a[0]: np.array(a[1:], dtype=float) \\\n",
    "                        for a in [line.strip().split('\\t') \\\n",
    "                                  for line in open('data/qmodel.tsv')] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.519 | Hits@   1: 0.519\n",
      "DCG@   5: 0.614 | Hits@   5: 0.697\n",
      "DCG@  10: 0.633 | Hits@  10: 0.755\n",
      "DCG@ 100: 0.665 | Hits@ 100: 0.906\n",
      "DCG@ 500: 0.674 | Hits@ 500: 0.981\n",
      "DCG@1000: 0.676 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 66\t26\t73\t58\t3\t82\t98\t96\t56\t36\t19\t13\t15\t32\t93\t48\t2\t55\t39\t91\t89\t74\t1\t70\t99\t9\t87\t62\t90\t67\t54\t42\t65\t76\t92...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = 'data/test_prep.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.019293891059\n",
      "-0.0287272135417\n",
      "0.0460561116536\n",
      "0.0852593315972\n",
      "0.0243055555556\n",
      "-0.0729031032986\n",
      "0.0...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.815464876786\n",
      "0.5\n",
      "0.815464876786\n",
      "0.333333333333\n",
      "0.54364325119\n",
      "0.710309917857\n",
      "0.1\n",
      "0.16309297...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n",
      "Task StarSpaceRanks: 66\t26\t73\t58\t3\t82\t98\t96\t56\t36\t19\t13\t15\t32\t93\t48\t2\t55\t39\t91\t89\t74\t1\t70\t99\t9\t87\t62\t90\t67\t54\t42\t65\t76\t92...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = 'jc@manoli.net' \n",
    "STUDENT_TOKEN = '04IJPQG67PZfgNVZ' \n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
